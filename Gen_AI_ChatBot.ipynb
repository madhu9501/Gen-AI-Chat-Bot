{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Give service account below permission: \\\n",
    "AI platform developer \\\n",
    "Cloud Translation API User\n",
    "\n",
    "2. Create a key for service account as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-core 0.3.5 requires langsmith<0.2.0,>=0.1.125, but you have langsmith 0.0.92 which is incompatible.\n",
      "langchain-google-vertexai 1.0.10 requires langchain-core<0.3,>=0.2.33, but you have langchain-core 0.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-api-python-client 1.8.0 requires google-api-core<2dev,>=1.13.0, but you have google-api-core 2.19.2 which is incompatible.\n",
      "langchain 0.0.340 requires langsmith<0.1.0,>=0.0.63, but you have langsmith 0.1.126 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-auth-oauthlib 1.2.1 requires google-auth>=2.15.0, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-aiplatform 1.67.1 requires google-auth<3.0.0dev,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-artifact-registry 1.11.5 requires google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-bigquery 3.25.0 requires google-auth<3.0.0dev,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-bigquery-storage 2.25.0 requires google-auth<3.0.0dev,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-language 2.14.0 requires google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-monitoring 2.22.2 requires google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-resource-manager 1.12.5 requires google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-storage 2.18.2 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "google-cloud-storage 2.18.2 requires google-auth<3.0dev,>=2.26.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-storage 2.18.2 requires google-cloud-core<3.0dev,>=2.3.0, but you have google-cloud-core 1.7.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-cloud-core 1.7.3 requires google-auth<2.0dev,>=1.24.0, but you have google-auth 2.15.0 which is incompatible.\n",
      "google-cloud-storage 2.18.2 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "google-cloud-storage 2.18.2 requires google-auth<3.0dev,>=2.26.1, but you have google-auth 2.15.0 which is incompatible.\n",
      "google-cloud-storage 2.18.2 requires google-cloud-core<3.0dev,>=2.3.0, but you have google-cloud-core 1.7.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-auth-oauthlib 1.2.1 requires google-auth>=2.15.0, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-aiplatform 1.67.1 requires google-auth<3.0.0dev,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-artifact-registry 1.11.5 requires google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-bigquery 3.25.0 requires google-auth<3.0.0dev,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-bigquery-storage 2.25.0 requires google-auth<3.0.0dev,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-language 2.14.0 requires google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-monitoring 2.22.2 requires google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-resource-manager 1.12.5 requires google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-storage 2.18.2 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "google-cloud-storage 2.18.2 requires google-auth<3.0dev,>=2.26.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-storage 2.18.2 requires google-cloud-core<3.0dev,>=2.3.0, but you have google-cloud-core 1.7.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install langchain==0.0.340 --quiet # Agent\n",
    "# ! pip install langsmith==0.1.84 --quiet\n",
    "!pip install -qU langchain-google-vertexai # gemini pro\n",
    "\n",
    "! pip install sentence-transformers --quiet # cross encoder\n",
    "\n",
    "! pip install numexpr --quiet # Math tool\n",
    "\n",
    "! pip install chromadb==0.4.13 --quiet # chroma vectore store\n",
    "\n",
    "! pip install google-cloud-bigquery[pandas] --quiet # BQ\n",
    "! pip install --upgrade google-cloud-aiplatform --quiet # vertex AI\n",
    "! pip install google-cloud-translate==2.0.1 --quiet # Translation\n",
    "! pip install google-auth==2.15.0 --quiet\n",
    "! pip install google-cloud-core==1.7.3 --quiet\n",
    "\n",
    "! pip install fuzzywuzzy --quiet # Fuzzy search for translator\n",
    "\n",
    "! pip install nltk --quiet # clean data for entity recognition\n",
    "\n",
    "! pip install sqlalchemy psycopg2-binary pandas --quiet # Postgres\n",
    "\n",
    "!pip install pyjwt # decode user token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Vertex configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YqvnALApSYA-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "VERTEX_PROJECT = \"project-name\"\n",
    "VERTEX_REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3AiF1f2Uxu6",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Postgres configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "Hk57x-JuSbCP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql, OperationalError, DatabaseError\n",
    "\n",
    "POSTGRES_CONN_PARAMS = {\n",
    "    \"database\" : 'databasename',\n",
    "    \"user\" : 'username',\n",
    "    \"password\" : 'pa55w0rd',\n",
    "    \"host\" : '00.000.00.000',\n",
    "    \"port\" : 0000,\n",
    "    \"schema\" : 'service'\n",
    "}\n",
    "\n",
    "def connect_db(database = POSTGRES_CONN_PARAMS['database'], user = POSTGRES_CONN_PARAMS['user'], password = POSTGRES_CONN_PARAMS['password'], host = POSTGRES_CONN_PARAMS['host'], port = POSTGRES_CONN_PARAMS['port']):\n",
    "    try:\n",
    "        logging.info(f\"connecting to the database: {database}\")\n",
    "        logging.debug(f\"database: {database}\")\n",
    "        logging.debug(f\"host: {host}\")\n",
    "        logging.debug(f\"port: {port}\")\n",
    "\n",
    "        conn_string = f\"dbname={database} user={user} password={password} host={host} port={port}\"\n",
    "        conn = psycopg2.connect(conn_string)\n",
    "        logging.info(f\"Successfully connected to the database: {database}\")\n",
    "        return conn\n",
    "    except psycopg2.DatabaseError as e:\n",
    "        logging.error(f\"Error connecting to database: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqUs77DoU0Ho",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wXnPJNPnVRbK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"keys.json\"\n",
    "vertexai.init(project=VERTEX_PROJECT, location=VERTEX_REGION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mY8OpGPMyeWZ",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MD08GTeYyftN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain.llms import VertexAI\n",
    "\n",
    "CODE_BISON = ChatVertexAI(model_name=\"codechat-bison\", max_output_tokens=1024)\n",
    "INTERPRETOR_MODEL = ChatVertexAI(max_output_tokens=1024)\n",
    "AGENT_MODEL = ChatVertexAI(max_output_tokens=1024)\n",
    "CHAT_BISON = ChatVertexAI(model_name=\"chat-bison@002\", max_output_tokens=1024)\n",
    "\n",
    "TEXT_BISON = VertexAI( model_name=\"text-bison@002\", max_output_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAI\n",
    "GEMINI_PRO = VertexAI(model_name=\"gemini-pro\")\n",
    "\n",
    "# from langchain_google_vertexai import ChatVertexAI as GoogleChatVertexAI\n",
    "# gemini_model = GoogleChatVertexAI(model_name=\"gemini-1.5-pro-001\", max_output_tokens=1024) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TABLE_DESCRIPTIONS = {\n",
    "    \"table_name1\": \"description\",\n",
    "    \"table_name2\": \"description\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "COLUMN_DESCRIPTIONS = {\"table_name1\":\n",
    "    {\"column1\": \"Unique identifier for the Aramco station.\",\n",
    "    \"column2\": \"Identifier for the tenant, referencing the tenants table.\",\n",
    "    },\n",
    "    \"table_name2\":\n",
    "    {\"column1\": \"Unique identifier for the bill.\",\n",
    "    \"column2\": \"Identifier for the type of bill.\",\n",
    "    },\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Table schema docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def fetch_table_names(cursor, schema):\n",
    "    \"\"\"Fetch all table names in the given schema.\"\"\"\n",
    "    logging.info(f\"Fetching table names from {schema}.\")\n",
    "\n",
    "    table_query = \"\"\"\n",
    "        SELECT table_name \n",
    "        FROM information_schema.tables \n",
    "        WHERE table_schema = %s\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cursor.execute(table_query, (schema,))\n",
    "        logging.info(f\"Fetched table names from {schema}.\")\n",
    "\n",
    "        return cursor.fetchall()\n",
    "    \n",
    "    except psycopg2.Error as e:\n",
    "        logging.error(f\"Error fetching table names: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_column_details(cursor, schema, table_name):\n",
    "    \"\"\"Fetch details of all columns in the specified table.\"\"\"\n",
    "    logging.info(f\"Fetching column deatils from {schema}, {table_name}.\")\n",
    "\n",
    "    column_query = \"\"\"\n",
    "    SELECT\n",
    "        c.column_name,\n",
    "        c.data_type,\n",
    "        tc.constraint_type,\n",
    "        ccu.table_name AS foreign_table_name,\n",
    "        ccu.column_name AS foreign_column_name\n",
    "    FROM\n",
    "        information_schema.columns AS c\n",
    "    LEFT JOIN\n",
    "        information_schema.key_column_usage AS kcu\n",
    "        ON c.table_name = kcu.table_name\n",
    "        AND c.column_name = kcu.column_name\n",
    "        AND c.table_schema = kcu.table_schema\n",
    "    LEFT JOIN\n",
    "        information_schema.table_constraints AS tc\n",
    "        ON kcu.constraint_name = tc.constraint_name\n",
    "        AND kcu.table_name = tc.table_name\n",
    "        AND kcu.table_schema = tc.table_schema\n",
    "    LEFT JOIN\n",
    "        information_schema.referential_constraints AS rc\n",
    "        ON kcu.constraint_name = rc.constraint_name\n",
    "    LEFT JOIN\n",
    "        information_schema.constraint_column_usage AS ccu\n",
    "        ON rc.unique_constraint_name = ccu.constraint_name\n",
    "    WHERE\n",
    "        c.table_schema = %s\n",
    "        AND c.table_name = %s\n",
    "    ORDER BY\n",
    "        c.ordinal_position;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cursor.execute(column_query, (schema, table_name))\n",
    "        return cursor.fetchall()\n",
    "    except psycopg2.Error as e:\n",
    "        logging.error(f\"Error fetching column details for table {table_name}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLES_USED = [\"table_name1\", \"table_name2\"]\n",
    "\n",
    "def get_parent_child_table_schemas(database, user, password, host, port, schema):\n",
    "    \"\"\"Fetch and organize parent child docs.\n",
    "    parent - table schemas \n",
    "    child - column schema\n",
    "    \"\"\"\n",
    "    logging.info(f\"Fetching table schema from databse {database}.\")\n",
    "    logging.debug(f\"database: {database}\")\n",
    "    logging.debug(f\"host: {host}\")\n",
    "    logging.debug(f\"port: {port}\")\n",
    "\n",
    "    child_docs = []\n",
    "    parent_docs = []\n",
    "\n",
    "    # Initialize connection to the database\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = connect_db(database, user, password, host, port)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Fetch all table names in the schema\n",
    "        tables = fetch_table_names(cursor, schema)\n",
    "\n",
    "        for table in tables:\n",
    "            table_name = table[0]\n",
    "            if table_name in TABLES_USED:\n",
    "                # Fetch column details for each table\n",
    "                columns = fetch_column_details(cursor, schema, table_name)\n",
    "\n",
    "                schema_fields = []\n",
    "                column_description = \"\"\n",
    "\n",
    "                for col in columns:\n",
    "                    # Placeholder for column descriptions (if available)\n",
    "                    if table_name in COLUMN_DESCRIPTIONS and col[0] in COLUMN_DESCRIPTIONS[table_name]:\n",
    "                        column_description = COLUMN_DESCRIPTIONS[table_name][col[0]]\n",
    "\n",
    "                    col_info = {\n",
    "                        \"column_name\": col[0],\n",
    "                        \"data_type\": col[1],\n",
    "                        \"constraint_type\": col[2],\n",
    "                        \"foreign_table_name\": col[3],\n",
    "                        \"foreign_column_name\": col[4],\n",
    "                        \"column_description\": column_description\n",
    "                    }\n",
    "                    schema_fields.append(col_info)\n",
    "\n",
    "                    col_info_col = {\n",
    "                        \"column_name\": col[0],\n",
    "                        \"column_description\": column_description\n",
    "                    }\n",
    "                    child_doc = {\n",
    "                        \"table_name\": table_name,\n",
    "                        \"schema\": schema,\n",
    "                        \"column_details\": col_info_col\n",
    "                    }\n",
    "                    child_docs.append(child_doc)\n",
    "\n",
    "                # Placeholder for table descriptions (if available)\n",
    "                table_description = \"\"\n",
    "                if table_name in TABLE_DESCRIPTIONS:\n",
    "                    table_description = TABLE_DESCRIPTIONS[table_name]\n",
    "\n",
    "                parent_doc = {\n",
    "                    \"table_name\": table_name,\n",
    "                    \"schema\": schema,\n",
    "                    \"table_description\": table_description,\n",
    "                    \"table_details\": schema_fields\n",
    "                }\n",
    "                parent_docs.append(parent_doc)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred in get_parent_child_table_schemas : {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        # Ensure that the connection is closed\n",
    "        if conn:\n",
    "            conn.close()\n",
    "            logging.info(\"Database connection closed.\")\n",
    "\n",
    "    return parent_docs, child_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parent_docs, child_docs =get_parent_child_table_schemas(database = POSTGRES_CONN_PARAMS['database'], user = POSTGRES_CONN_PARAMS['user'], password = POSTGRES_CONN_PARAMS['password'], host = POSTGRES_CONN_PARAMS['host'], port = POSTGRES_CONN_PARAMS['port'], schema= POSTGRES_CONN_PARAMS['schema'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Entity Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "# Get all columns of type string and 3 sample data as context #######\n",
    "###########################################################################\n",
    "def get_string_column_with_sample(conn, table_name):\n",
    "    string_columns_with_samples = []\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            # Get the columns of the specified table\n",
    "            cursor.execute(sql.SQL(\"SELECT column_name, data_type FROM information_schema.columns WHERE table_name = %s\"), [table_name])\n",
    "            columns = cursor.fetchall()\n",
    "\n",
    "            for column_name, data_type in columns:\n",
    "                if data_type == \"character varying\" or data_type == \"text\":\n",
    "                    # Query to get 3 samples from the column\n",
    "                    query = sql.SQL(\"SELECT {column} FROM {table} LIMIT 3\").format(\n",
    "                        column=sql.Identifier(column_name),\n",
    "                        table=sql.Identifier(table_name)\n",
    "                    )\n",
    "                    cursor.execute(query)\n",
    "                    samples = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "                    string_columns_with_samples.append({\n",
    "                        \"table\": table_name,\n",
    "                        \"column\": column_name,\n",
    "                        \"samples\": samples\n",
    "                    })\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error retrieving string columns from table {table_name}: {e}\")\n",
    "\n",
    "    return string_columns_with_samples\n",
    "\n",
    "def identify_proper_noun_columns(column_samples, llm):\n",
    "    formatted_columns = []\n",
    "    for col in column_samples:\n",
    "        if col['samples'] is None:\n",
    "            samples_str = \"\"  # Handle NoneType gracefully\n",
    "        else:\n",
    "            # Filter out NoneType values and convert to strings if necessary\n",
    "            samples = [str(s) for s in col['samples'] if s is not None]\n",
    "            samples_str = ', '.join(samples)\n",
    "        formatted_columns.append(f\"Column: {col['column']}, Samples: {samples_str}\")\n",
    "    \n",
    "    formatted_columns_str = \"\\n\".join(formatted_columns)\n",
    "\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"\"\"Given the list of columns in a table with their sample data, return all of the columns that likely contain proper nouns (names, locations, etc.).\n",
    "        columns and samples:\n",
    "        {columns}\n",
    "        result (just the column names, separated by commas):\"\"\"\n",
    "    )\n",
    "    \n",
    "    output_parser = CommaSeparatedListOutputParser()\n",
    "    chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)\n",
    "\n",
    "    noun_columns = chain.invoke({\"columns\": formatted_columns_str})\n",
    "    noun_columns_list = noun_columns['text'].strip().split(\", \")  # Split into list\n",
    "\n",
    "    return noun_columns_list\n",
    "\n",
    "###########################################################################\n",
    "# Get all noun data that the LLM can refer to for making better decisions\n",
    "###########################################################################\n",
    "def get_all_noun_data(llm):\n",
    "    conn = connect_db()  # Connect to the PostgreSQL database\n",
    "    all_data = set()\n",
    "    all_noun_map = []  # Initialize as a list\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(\"SELECT table_name FROM information_schema.tables WHERE table_schema = %s\", [POSTGRES_CONN_PARAMS['schema']])\n",
    "            tables = fetch_table_names(cursor, POSTGRES_CONN_PARAMS['schema'])\n",
    "\n",
    "            for table in tables:\n",
    "                table_name = table[0]\n",
    "                string_columns_with_samples = get_string_column_with_sample(conn, table_name)\n",
    "\n",
    "                noun_columns_list = identify_proper_noun_columns(string_columns_with_samples, llm)\n",
    "\n",
    "                for noun_column in noun_columns_list:\n",
    "                    # Retrieve actual noun data from the table\n",
    "                    query = sql.SQL(\"SELECT {column} FROM {table}\").format(\n",
    "                        column=sql.Identifier(noun_column),\n",
    "                        table=sql.Identifier(table_name)\n",
    "                    )\n",
    "                    cursor.execute(query)\n",
    "                    results = cursor.fetchall()\n",
    "\n",
    "                    for row in results:\n",
    "                        column_value = row[0]\n",
    "                        if column_value is not None:\n",
    "                            all_data.add(column_value)\n",
    "                            all_noun_map.append({column_value: {\"table\": table_name, \"column\": noun_column}})  # Append to list as a dict\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error retrieving data: {e}\")\n",
    "\n",
    "    finally:\n",
    "        conn.close()  # Close the database connection\n",
    "\n",
    "    return list(all_data), all_noun_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_noun_data, noun_maps = get_all_noun_data(TEXT_BISON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Translated Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import translate_v2 as translate\n",
    "\n",
    "######################\n",
    "\"\"\"\n",
    "Basic Translator \n",
    "1. For getting initial translation of the user query \n",
    "2. For getting Entity translations for fuzzy and vector search # NEEDS human verification\n",
    "above will be provided as context to LLM in advanced translator\n",
    "\n",
    "IP:\n",
    "1. Text to be translated \n",
    "2. Taregt language to be translate to\n",
    "\n",
    "OP:\n",
    "Translated text\n",
    "\n",
    "\"\"\"\n",
    "######################\n",
    "def translate_text(text: str, tgt: str) -> str:\n",
    "    try:\n",
    "        logging.debug(f\"translating text: {text[:50]}... to {tgt}\")\n",
    "        # Initialize Google Cloud Translation client\n",
    "        translate_client = translate.Client()\n",
    "\n",
    "        # Ensure text is in string format\n",
    "        if isinstance(text, bytes):\n",
    "            text = text.decode(\"utf-8\")\n",
    "\n",
    "        # Perform translation\n",
    "        result = translate_client.translate(text, target_language=tgt)\n",
    "        translated_text = result.get(\"translatedText\", \"\")\n",
    "\n",
    "        # Log successful translation\n",
    "        logging.info(f\"Successfully translated text to: {translated_text[:50]}...\")\n",
    "\n",
    "        return translated_text\n",
    "    except Exception as e:\n",
    "        # Log error\n",
    "        logging.error(f\"Error translating text: {text[:50]}... to {tgt} - {e}\")\n",
    "        return \"\"\n",
    "######################\n",
    "\"\"\"\n",
    "Translate Entity:(SRF)\n",
    "Provides tranlsation of entities. \n",
    "Used for mapping translated entites to entities in database(en)\n",
    "\n",
    "IP:\n",
    "1. List of entities \n",
    "2. Taregt language to be translate to\n",
    "\n",
    "OP:\n",
    "1. List of dicts [{en:ar}, {en:ar}] \n",
    "2. list of translated entities\n",
    "\"\"\"\n",
    "######################\n",
    "\n",
    "def get_entity_translations(datas, tgt_lang):\n",
    "    logging.info(\"Creating entity : translations map\")\n",
    "    logging.debug(f\"Target language : {tgt_lang}\")\n",
    "\n",
    "    translate_map_list = []\n",
    "    translated_texts_list = []\n",
    "\n",
    "    for data in datas:\n",
    "        if not data:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Translate text and build map\n",
    "            translated_texts = translate_text(data, tgt_lang)\n",
    "            if translated_texts:\n",
    "                translate_map = {data: translated_texts}\n",
    "                translated_texts_list.append(translated_texts)\n",
    "                translate_map_list.append(translate_map)\n",
    "            else:\n",
    "                logging.warning(f\"Translation failed for text: {data[:50]}...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Log error during translation\n",
    "            logging.error(f\"Error processing translation for {data[:50]}...: {e}\")\n",
    "    logging.info(\"Completed entity : translations map\")\n",
    "    return translate_map_list, translated_texts_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "entity_translate_map, translated_texts_list = get_entity_translations(all_noun_data, \"ar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Entity to split words docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######################\n",
    "\"\"\"\n",
    "Creats words to entity map: \n",
    "Splits given entity into words and maps each word to its entity\n",
    "\n",
    "IP:\n",
    "1. List of entities\n",
    "\n",
    "OP:\n",
    "1. Dict {entity: [words, in, entity], entity: [words, in, entity], ... }\n",
    "2. list of split words [words, in, all, entity]\n",
    "\"\"\"\n",
    "######################\n",
    "\n",
    "def get_entity_words_map(entities_list):\n",
    "    logging.info(\"Splittig entity and mapping each word to its Entity\")\n",
    "\n",
    "    word_entity_map = {}\n",
    "    all_split_words = set()\n",
    "\n",
    "    for text in entities_list:\n",
    "        if not text:  # Check for empty strings\n",
    "            logging.warning(\"Encountered empty string in entities_list.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Split text into words and update maps\n",
    "            split_texts = text.split()\n",
    "            all_split_words.update(split_texts)\n",
    "            word_entity_map[text] = split_texts\n",
    "\n",
    "            logging.info(f\"Processed entity: '{text}' -> Words: {split_texts}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing entity '{text}': {e}\")\n",
    "\n",
    "    return word_entity_map, all_split_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Entity context for sql query\n",
    "##################################\n",
    "en_word_entity_map, en_all_split_words = get_entity_words_map(all_noun_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract name form email (NOT USED)\n",
    "def extract_name_from_email(email):\n",
    "    name = re.sub(r'@.*', '', email)\n",
    "    name = re.sub(r'[^a-zA-Z0-9]', '', name)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "Entity context for traslation\n",
    "#################################\n",
    "translated_texts_dict, all_translated_words = get_entity_words_map(translated_texts_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Vector Store "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Vector stores - Table Schema and Entities \n",
    "##############################################\n",
    "\n",
    "#################################\n",
    "\"\"\"\n",
    "Create custom embeding finction using multi-lingual model\n",
    "\"\"\"\n",
    "#################################\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from vertexai.language_models import TextEmbeddingModel, TextEmbeddingInput\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "class MyEmbeddingFunction(EmbeddingFunction):\n",
    "    def __init__(self, model_id, task):\n",
    "        try:\n",
    "            self.model = TextEmbeddingModel.from_pretrained(model_id)\n",
    "            self.task = task\n",
    "            logging.info(f\"Initialized embedding model: {model_id} for task: {task}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error initializing embedding model '{model_id}': {e}\")\n",
    "            raise  # Reraise the exception to avoid using an uninitialized model\n",
    "\n",
    "    def embed_documents(self, input: Documents) -> Embeddings:\n",
    "        max_batch_size = 250  # Batch to max limit -> 250\n",
    "        embeddings = []\n",
    "        \n",
    "        try:\n",
    "            for i in range(0, len(input), max_batch_size):\n",
    "                batch = input[i:i + max_batch_size]\n",
    "                inputs = [TextEmbeddingInput(text, self.task) for text in batch]\n",
    "                \n",
    "                # Get embeddings for the batch\n",
    "                embedding_result = self.model.get_embeddings(inputs)\n",
    "                batch_embeddings = [embedding.values for embedding in embedding_result]\n",
    "                embeddings.extend(batch_embeddings)\n",
    "\n",
    "                logging.info(f\"Processed batch {i // max_batch_size + 1}: {len(batch)} documents embedded.\")\n",
    "                \n",
    "            return embeddings\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error embedding documents: {e}\")\n",
    "            return []  # Return an empty list in case of error\n",
    "\n",
    "    def embed_query(self, query: str) -> Embeddings:\n",
    "        try:\n",
    "            inputs = [TextEmbeddingInput(query, self.task)]\n",
    "            embedding_result = self.model.get_embeddings(inputs)\n",
    "            embeddings = [embedding.values for embedding in embedding_result]\n",
    "            logging.info(f\"Processed query: '{query}' -> Embedding obtained.\")\n",
    "            return embeddings[0] if embeddings else []  # Return first embedding or empty\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error embedding query '{query}': {e}\")\n",
    "            return []  # Return an empty list in case of error\n",
    "\n",
    "##############################\n",
    "# Model, task type and creating embedding function\n",
    "##############################\n",
    "EMBEDDING_MODEL = \"text-multilingual-embedding-002\"\n",
    "EMBEDDING_TASK = \"RETRIEVAL_DOCUMENT\"\n",
    "\n",
    "try:\n",
    "    embedding_function = MyEmbeddingFunction(EMBEDDING_MODEL, EMBEDDING_TASK)\n",
    "except Exception as e:\n",
    "    logging.critical(f\"Failed to create embedding function: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Translated entity vector store for Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ##############################\n",
    "# \"\"\"\n",
    "# Create vector store retriever for retrieving relevant {en:ar} entity map for user query  \n",
    "# \"\"\"\n",
    "# #############################\n",
    "# Convert entity translate map to string representation\n",
    "entity_translate_map_str = [str(doc) for doc in entity_translate_map]\n",
    "\n",
    "# Attempt to delete the existing collection and handle exceptions\n",
    "try: \n",
    "    entity_translate_vector_store.delete_collection()\n",
    "    logging.info(\"Deleted existing collection in vector store.\")\n",
    "except Exception as e:\n",
    "    logging.warning(f\"No need to clean the vector store or error occurred: {e}\")\n",
    "\n",
    "# Create a new vector store from the texts\n",
    "try:\n",
    "    entity_translate_vector_store = Chroma.from_texts(\n",
    "        entity_translate_map_str, \n",
    "        embedding=embedding_function,\n",
    "        collection_name=\"entity_translate_map_collection\",\n",
    "        persist_directory=\"entity_translate_map_collection\",\n",
    "    )\n",
    "    logging.info(\"Successfully created a new vector store from texts.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error creating vector store: {e}\")\n",
    "    raise  # Reraise the exception for further handling if necessary\n",
    "\n",
    "# Attempt to persist the collection to disk\n",
    "try:\n",
    "    entity_translate_vector_store.persist()\n",
    "    logging.info(\"Persisted the collection to disk successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error persisting the collection: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_entity_translate_retriever(count=8):\n",
    "    try:\n",
    "        entity_translate_retriever = entity_translate_vector_store.as_retriever(search_kwargs={'k': count})\n",
    "        logging.info(f\"Created entity translate retriever with retrive count: {count}\")\n",
    "        return entity_translate_retriever\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating entity translate retriever: {e}\")\n",
    "        return None  # Return None or handle as appropriate\n",
    "\n",
    "# Specify the directory to load the persisted vector store\n",
    "translation_persist_directory = 'entity_translate_map_collection'\n",
    "\n",
    "# Attempt to load the vector store from the persisted data\n",
    "try:\n",
    "    entity_translate_vector_store = Chroma(\n",
    "        collection_name=\"entity_translate_map_collection\", \n",
    "        persist_directory=translation_persist_directory, \n",
    "        embedding_function=embedding_function\n",
    "    )\n",
    "    logging.info(\"Successfully loaded the vector store from persisted data.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading vector store: {e}\")\n",
    "    raise  # Reraise the exception for further handling if necessary\n",
    "\n",
    "# Use the retriever from the loaded store\n",
    "entity_translate_retriever = get_entity_translate_retriever()\n",
    "\n",
    "if entity_translate_retriever is None:\n",
    "    logging.critical(\"Failed to create entity translate retriever. Please check the logs for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Table vector for SQL query generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Table Schema vector stores\n",
    "##############################################\n",
    "import json\n",
    "\n",
    "# Attempt to delete the existing collection and handle exceptions\n",
    "try:\n",
    "    schema_vector_store.delete_collection()\n",
    "    logging.info(\"Deleted existing schema collection in vector store.\")\n",
    "except Exception as e:\n",
    "    logging.warning(f\"No need to clean the vector store or error occurred: {e}\")\n",
    "\n",
    "# Convert child documents to string representation\n",
    "try:\n",
    "    child_docs_as_strings = [json.dumps(doc) for doc in child_docs]\n",
    "    logging.info(\"Converted child documents to string format.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error converting child documents to strings: {e}\")\n",
    "    raise  # Reraise the exception for further handling if necessary\n",
    "\n",
    "# Attempt to create a new vector store from the texts\n",
    "try:\n",
    "    schema_vector_store = Chroma.from_texts(\n",
    "        child_docs_as_strings, \n",
    "        embedding=embedding_function,\n",
    "        collection_name=\"schema_collection\",\n",
    "        persist_directory=\"schema_collection\"\n",
    "    )\n",
    "    logging.info(\"Successfully created schema vector store from texts.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error creating schema vector store: {e}\")\n",
    "    raise  # Reraise the exception for further handling if necessary\n",
    "\n",
    "# Attempt to persist the collection to disk\n",
    "try:\n",
    "    schema_vector_store.persist()\n",
    "    logging.info(\"Persisted the schema collection to disk successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error persisting the schema collection: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_schema_retriever(count=10):\n",
    "    try:\n",
    "        schema_retriever = schema_vector_store.as_retriever(search_kwargs={'k': count})\n",
    "        logging.info(f\"Created schema retriever with retrive count: {count}\")\n",
    "        return schema_retriever\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating schema retriever: {e}\")\n",
    "        return None  # Return None or handle as appropriate\n",
    "\n",
    "# Specify the directory to load the persisted vector store\n",
    "schema_persist_directory = 'schema_collection'\n",
    "\n",
    "# Attempt to load the vector store from the persisted data\n",
    "try:\n",
    "    schema_vector_store = Chroma(\n",
    "        collection_name=\"schema_collection\", \n",
    "        persist_directory=schema_persist_directory, \n",
    "        embedding_function=embedding_function\n",
    "    )\n",
    "    logging.info(\"Successfully loaded the schema vector store from persisted data.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading schema vector store: {e}\")\n",
    "    raise  # Reraise the exception for further handling if necessary\n",
    "\n",
    "# Use the retriever from the loaded store\n",
    "schema_retriever = get_schema_retriever()\n",
    "\n",
    "if schema_retriever is None:\n",
    "    logging.critical(\"Failed to create schema retriever. Please check the logs for details.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Entity vector store for SQL query generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "##############################################\n",
    "# Entities vector stores\n",
    "##############################################\n",
    "\n",
    "# Attempt to delete the existing noun vector store collection and handle exceptions\n",
    "try:\n",
    "    noun_vector_store.delete_collection()\n",
    "    logging.info(\"Deleted existing noun collection in vector store.\")\n",
    "except Exception as e:\n",
    "    logging.warning(f\"No need to clean the vector store or an error occurred: {e}\")\n",
    "\n",
    "# Attempt to create a new noun vector store from the texts\n",
    "try:\n",
    "    noun_vector_store = Chroma.from_texts(\n",
    "        all_noun_data,\n",
    "        embedding=embedding_function,\n",
    "        collection_name=\"noun_collection\",\n",
    "        persist_directory=\"noun_collection\"\n",
    "    )\n",
    "    logging.info(\"Successfully created noun vector store from texts.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error creating noun vector store: {e}\")\n",
    "    raise  # Reraise the exception for further handling if necessary\n",
    "\n",
    "# Attempt to persist the collection to disk\n",
    "try:\n",
    "    noun_vector_store.persist()\n",
    "    logging.info(\"Persisted the noun collection to disk successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error persisting the noun collection: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_retriever(count=10):\n",
    "    try:\n",
    "        noun_retriever = noun_vector_store.as_retriever(search_kwargs={'k': count})\n",
    "        logging.info(f\"Noun retriever created with retrive cpount {count}.\")\n",
    "        return noun_retriever\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating noun retriever: {e}\")\n",
    "        raise  # Reraise the exception for further handling if necessary\n",
    "\n",
    "noun_persist_directory = 'noun_collection'\n",
    "\n",
    "# Attempt to load the vector store from the persisted data\n",
    "try:\n",
    "    noun_vector_store = Chroma(\n",
    "        collection_name=\"noun_collection\",\n",
    "        persist_directory=noun_persist_directory,\n",
    "        embedding_function=embedding_function\n",
    "    )\n",
    "    logging.info(\"Successfully loaded noun vector store from persisted data.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading noun vector store: {e}\")\n",
    "    raise  # Reraise the exception for further handling if necessary\n",
    "\n",
    "# Use the retriever from the loaded store\n",
    "try:\n",
    "    noun_retriever = get_noun_retriever()\n",
    "    logging.info(\"Successfully retrieved noun retriever.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error retrieving noun retriever: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_entity_schema(entities):\n",
    "    noun_rows = []\n",
    "    \n",
    "    try:\n",
    "        conn = connect_db()\n",
    "        cursor = conn.cursor()\n",
    "        logging.info(\"Database connection established successfully.\")\n",
    "        \n",
    "        for entity in entities:\n",
    "            for noun_map in noun_maps:\n",
    "                if entity in noun_map:\n",
    "                    schema = noun_map[entity]['schema']\n",
    "                    table_name = noun_map[entity]['table']\n",
    "                    column_name = noun_map[entity]['column']\n",
    "                    \n",
    "                    sql_query = f\"\"\"SELECT * FROM \"{schema}\".\"{table_name}\" WHERE \"{column_name}\" = %s\"\"\"\n",
    "                    cursor.execute(sql_query, (entity,))  # Use parameterized query to prevent SQL injection\n",
    "\n",
    "                    columns = [desc[0] for desc in cursor.description]  # Get column names from cursor\n",
    "                    for row in cursor.fetchall():\n",
    "                        temp_dict = {}\n",
    "                        temp_dict[\"query\"] = sql_query\n",
    "                        temp_dict[\"query_result\"] = dict(zip(columns, row))  # Map column names to row values\n",
    "                        noun_rows.append(temp_dict)\n",
    "        \n",
    "        logging.info(f\"Processed {len(entities)} entities and retrieved their schemas successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while retrieving entity schema: {e}\")\n",
    "        raise  # Reraise the exception for further handling if necessary\n",
    "    \n",
    "    finally:\n",
    "        # Ensure cursor and connection are closed properly\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "            logging.info(\"Cursor closed.\")\n",
    "        if conn:\n",
    "            conn.close()\n",
    "            logging.info(\"Database connection closed.\")\n",
    "    \n",
    "    return noun_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Entity retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Retriver for Entities \n",
    "##############################################\n",
    "\n",
    "def get_noun_results(question):\n",
    "    noun_rows = []\n",
    "    \n",
    "    try:\n",
    "        logging.info(f\"Retrieving relevant documents for question: {question}\")\n",
    "        results = noun_retriever.get_relevant_documents(question)\n",
    "\n",
    "        if not results:\n",
    "            logging.warning(\"No relevant documents found.\")\n",
    "            return noun_rows  # Return an empty list if no results\n",
    "\n",
    "        entities = {result.page_content for result in results}\n",
    "        logging.info(f\"Identified entities: {entities}\")\n",
    "\n",
    "        noun_rows = get_entity_schema(entities) \n",
    "        logging.info(f\"Retrieved schema for {len(entities)} entities.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while retrieving noun results: {e}\")\n",
    "        raise  # Reraise the exception for further handling if necessary\n",
    "    \n",
    "    return noun_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Entity retriever for tranlsated entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_schema(entities_list):\n",
    "    noun_rows = []\n",
    "    \n",
    "    try:\n",
    "        logging.info(\"Processing entities to retrieve schema.\")\n",
    "        \n",
    "        # Use a set to avoid duplicates\n",
    "        entities = set(entities_list)  \n",
    "        logging.info(f\"Identified unique entities: {entities}\")\n",
    "\n",
    "        # Get the entity schema\n",
    "        noun_rows = get_entity_schema(entities)\n",
    "\n",
    "        logging.info(f\"Retrieved schema for {len(entities)} entities.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while retrieving schema: {e}\")\n",
    "        raise  # Reraise the exception for further handling if necessary\n",
    "    \n",
    "    return noun_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Table schema retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Retriver for table schema - Auto merging retrieval\n",
    "##############################################\n",
    "\n",
    "# Create a lookup for column details to table name\n",
    "column_details_to_table_name_lookup = {json.dumps(doc['column_details']): doc['table_name'] for doc in child_docs}\n",
    "\n",
    "def get_relevant_parent_docs(question):\n",
    "    retrieved_tables = []\n",
    "    \n",
    "    try:\n",
    "        logging.info(\"Retrieving relevant parent documents based on the provided question.\")\n",
    "        \n",
    "        # Retrieve relevant documents from schema_retriever\n",
    "        results = schema_retriever.get_relevant_documents(question)\n",
    "        logging.info(f\"Retrieved {len(results)} documents from the schema retriever.\")\n",
    "        \n",
    "        # Find table names from retrieved results\n",
    "        retrieved_table_ids = set()\n",
    "        for result in results:\n",
    "            # Check if any of the JSON column details from child_docs are in the result's page content\n",
    "            for column_details_json in column_details_to_table_name_lookup:\n",
    "                if column_details_json in result.page_content:\n",
    "                    retrieved_table_ids.add(column_details_to_table_name_lookup[column_details_json])\n",
    "        \n",
    "        logging.info(f\"Identified table names: {retrieved_table_ids}\")\n",
    "        \n",
    "        # Retrieve relevant parent documents\n",
    "        retrieved_tables = [doc for doc in parent_docs if doc['table_name'] in retrieved_table_ids]\n",
    "        logging.info(f\"Retrieved {len(retrieved_tables)} relevant parent documents.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while retrieving relevant parent documents: {e}\")\n",
    "        raise  # Reraise the exception for further handling if necessary\n",
    "\n",
    "    return retrieved_tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Summarizer (Not Used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# summary = summarizer(res_str, max_length=100, min_length=30, do_sample=False,length_penalty=2.0, num_beams=4)\n",
    "\n",
    "\"\"\"\n",
    "length_penalty: A value >1.0 encourages the model to generate longer sequences, which can result in more detailed summaries.\n",
    "num_beams: Beam search parameter that can improve the quality of the summary. More beams can lead to a more thorough exploration of possible summaries.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Re-Ranker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "# Cross Encoder Model for re-ranking\n",
    "###################################\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "CROSS_ENCODER_MODEL = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "# ReRanker for retrieved docs\n",
    "###################################\n",
    "\n",
    "def re_rank_doc(query, docs, cross_encoder, count=3):\n",
    "    if not docs:\n",
    "        logging.warning(\"No documents provided for re-ranking.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        logging.info(\"Starting the re-ranking process.\")\n",
    "\n",
    "        # Create pairs of the query and each document\n",
    "        pairs = [[query, doc] for doc in docs]\n",
    "        logging.info(f\"Generated {len(pairs)} pairs for prediction.\")\n",
    "\n",
    "        # Get scores from the cross encoder\n",
    "        scores = cross_encoder.predict(pairs)\n",
    "        logging.info(\"Scores have been predicted successfully.\")\n",
    "\n",
    "        # Sort indices based on scores\n",
    "        sorted_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
    "        logging.info(f\"Sorted document indices based on scores.\")\n",
    "\n",
    "        # Select top documents based on the sorted indices\n",
    "        top_docs = [docs[i] for i in sorted_indices[:count]]\n",
    "        logging.info(f\"Selected top {count} documents for the final output.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during the re-ranking process: {e}\")\n",
    "        raise  # Reraise the exception for further handling if necessary\n",
    "\n",
    "    return top_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Fuzzy search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "\n",
    "#############################\n",
    "\"\"\"\n",
    "Perform fuzzy search\n",
    "\n",
    "IP:\n",
    "1. Word to be searched\n",
    "2. list of words to be searched from\n",
    "3. count of how many relevant search to retrieve\n",
    "4. score threshold\n",
    "\n",
    "OP:\n",
    "Top matches with score higher than 60\n",
    "\"\"\"\n",
    "#############################\n",
    "def fuzzy_search(search_word, search_from, limit=3, score=90):\n",
    "    try:\n",
    "        logging.info(f\"Starting fuzzy search for '{search_word}' with limit {limit} and score threshold {score}.\")\n",
    "        fuzzy_matches = process.extract(search_word, search_from, limit=limit)\n",
    "        \n",
    "        top_matches = [match[0] for match in fuzzy_matches if match[1] > score]\n",
    "        logging.info(f\"Found {len(fuzzy_matches)} matches. Filtered down to {len(top_matches)} based on score threshold.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during fuzzy search: {e}\")\n",
    "        raise  # Reraise the exception for further handling if necessary\n",
    "\n",
    "    return top_matches\n",
    "\n",
    "\n",
    "#############################\n",
    "\"\"\"\n",
    "Map fuzzy search results to entities in DB\n",
    "\n",
    "IP:\n",
    "1. List of word to be searched\n",
    "2. list of words to be searched from\n",
    "3. count of how many relevant search to retrieve\n",
    "4. score threshold\n",
    "\n",
    "OP:\n",
    "1. list of fuzzy search results to all input words\n",
    "\"\"\"\n",
    "#############################\n",
    "def get_matches(word_list, search_from, limit=3, score=90):\n",
    "    top_matches_set = set()\n",
    "\n",
    "    try:\n",
    "        logging.info(f\"Starting to get matches for {len(word_list)} words.\")\n",
    "        for word in word_list:\n",
    "            logging.info(f\"Searching matches for word: '{word}'\")\n",
    "            matches = fuzzy_search(word, search_from, limit, score)\n",
    "            top_matches_set.update(matches)\n",
    "        \n",
    "        top_matches = list(top_matches_set)\n",
    "        logging.info(f\"Found a total of {len(top_matches)} unique matches across all words.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while getting matches: {e}\")\n",
    "        raise  # Reraise the exception for further handling if necessary\n",
    "\n",
    "    return top_matches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Context "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Context from entity for SQL query generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "\"\"\"\n",
    "Map fuzzy search results to entities\n",
    "\n",
    "IP:\n",
    "1. List of words to map\n",
    "2. List of dict containg all word to entity maps [{Entity: [words, in, entity]}, ... ]\n",
    "\n",
    "OP:\n",
    "1. fuzzy search results mapped to noun maps - [entity1, entity2, ....]\n",
    "\"\"\"\n",
    "#############################\n",
    "from collections import defaultdict\n",
    "\n",
    "def build_reverse_lookup(entity_words_map):\n",
    "    reverse_map = defaultdict(set)  # A dictionary to map words to entities\n",
    "    try:\n",
    "        logging.info(\"Building reverse lookup map.\")\n",
    "        for entity, words in entity_words_map.items():  # Iterate through the dictionary directly\n",
    "            for word in words:\n",
    "                reverse_map[word].add(entity)  # Map word to the corresponding entity\n",
    "        logging.info(\"Successfully built the reverse lookup map.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while building reverse lookup map: {e}\")\n",
    "        raise  # Reraise the exception for further handling if necessary\n",
    "\n",
    "    return reverse_map\n",
    "\n",
    "\n",
    "\n",
    "def map_word_to_entity(word_list, reverse_map):\n",
    "    entities = set()\n",
    "    try:\n",
    "        logging.info(f\"Mapping words to entities for word list of size: {len(word_list)}.\")\n",
    "        for word in word_list:\n",
    "            if word in reverse_map:  # Direct lookup in reverse map\n",
    "                entities.update(reverse_map[word])\n",
    "        logging.info(f\"Mapping complete. Found {len(entities)} unique entities.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while mapping words to entities: {e}\")\n",
    "        raise  # Reraise the exception for further handling if necessary\n",
    "\n",
    "    return entities\n",
    "\n",
    "reverse_map = build_reverse_lookup(en_word_entity_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Context from enity for translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "\"\"\"\n",
    "Map fuzzy search results to entities in DB\n",
    "\n",
    "IP:\n",
    "1. List of words to map\n",
    "2. List of dict containg all word to entity maps {Entity: [words, in, entity], ... }\n",
    "3. list of dict containg [{en:ar},..] maps\n",
    "\n",
    "OP:\n",
    "1. fuzzy search results mapped to {en:ar}\n",
    "\"\"\"\n",
    "#############################\n",
    "def build_translation_lookup(translate_map):\n",
    "    try:\n",
    "        logging.info(\"Building translation lookup.\")\n",
    "        translation_lookup = {v: k for ele in translate_map for k, v in ele.items()}\n",
    "        logging.info(\"Successfully built the translation lookup.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while building translation lookup: {e}\")\n",
    "        raise  # Reraise the exception for further handling if necessary\n",
    "\n",
    "    return translation_lookup\n",
    "\n",
    "translation_lookup = build_translation_lookup(entity_translate_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def map_fuzzy_to_entity_translator_context(word_list, entity_words_map, translation_lookup=translation_lookup):\n",
    "    if translation_lookup is None:\n",
    "        logging.warning(\"No translation lookup provided; proceeding without it.\")\n",
    "        translation_lookup = {}\n",
    "\n",
    "    try:\n",
    "        logging.info(\"Mapping fuzzy words to entities.\")\n",
    "        \n",
    "        # Step 1: Find matching entities for the word list\n",
    "        entities = map_word_to_entity(word_list, entity_words_map)\n",
    "        \n",
    "        # Step 2: Map entities to their corresponding translations using the lookup\n",
    "        word_entity_map = [\n",
    "            {translation_lookup[entity]: entity} for entity in entities if entity in translation_lookup\n",
    "        ]\n",
    "        \n",
    "        logging.info(f\"Mapping complete. Found {len(word_entity_map)} translation mappings.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while mapping fuzzy words to entities: {e}\")\n",
    "        raise  # Reraise the exception for further handling if necessary\n",
    "\n",
    "    return word_entity_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Context from query for sql query generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def get_documents(question, entities_list):\n",
    "    output = \"\"\n",
    "    top_tabel_schema_str = \"\"\n",
    "    noun_doc_str = \"\"\n",
    "    noun_doc_str_prompt = \"\"\n",
    "\n",
    "    try:\n",
    "        ################################# RAG table schema from query #############################\n",
    "        logging.info(\"Retrieving relevant parent documents for the table schema.\")\n",
    "        retrieved_parent_docs = get_relevant_parent_docs(question)\n",
    "        retrieved_tables_str = [json.dumps(doc) for doc in retrieved_parent_docs]\n",
    "        logging.info(f\"Retrieved {len(retrieved_parent_docs)} parent documents.\")\n",
    "\n",
    "        ############################# Re-Rank & filter #############################\n",
    "        if retrieved_tables_str:\n",
    "            logging.info(\"Re-ranking table schema documents.\")\n",
    "            top_tabel_schema = re_rank_doc(question, retrieved_tables_str, CROSS_ENCODER_MODEL, 2)\n",
    "            logging.debug(f\"Length of table schema documents after Re-ranking is {len(top_tabel_schema)}\")\n",
    "            logging.debug(f\"Table schema documents after Re-ranking is {top_tabel_schema}\")\n",
    "\n",
    "            logging.info(\"Filtering table schema documents.\")\n",
    "            filter_retrieved_parent_docs = filter_context(question, top_tabel_schema, CODE_BISON)\n",
    "            logging.debug(f\"Length of table schema documents after filtering is {len(filter_retrieved_parent_docs)}\")\n",
    "            logging.debug(f\"Table schema documents after filtering is {filter_retrieved_parent_docs}\")\n",
    "\n",
    "            if filter_retrieved_parent_docs:\n",
    "                top_tabel_schema_str = '\\n\\n'.join([str(doc) for doc in filter_retrieved_parent_docs])\n",
    "            else:\n",
    "                top_tabel_schema_str = '\\n\\n'.join([str(doc) for doc in top_tabel_schema])\n",
    "\n",
    "        ################################# RAG entity from query #############################  \n",
    "        logging.info(\"Retrieving noun documents from PostgreSQL.\")\n",
    "        retrieved_noun_docs = get_noun_results(question) \n",
    "        retrieved_noun_docs_str = [str(doc) for doc in retrieved_noun_docs]\n",
    "        logging.info(f\"Retrieved {len(retrieved_noun_docs)} noun documents.\")\n",
    "\n",
    "        if entities_list:\n",
    "            logging.info(\"Getting schema for entities list.\")\n",
    "            entities_list_docs = get_schema(entities_list)\n",
    "            entities_list_docs_str = [str(doc) for doc in entities_list_docs]\n",
    "            combined_noun_docs_set = set(entities_list_docs_str + retrieved_noun_docs_str)\n",
    "            retrieved_noun_docs_str = list(combined_noun_docs_set)\n",
    "\n",
    "        ############################# Re-Rank & filter #############################\n",
    "        if retrieved_noun_docs_str:\n",
    "            logging.info(\"Re-ranking noun documents.\")\n",
    "            top_entity_count = 2\n",
    "            logging.info(f\"Max Re-ranked noun documents len {top_entity_count}.\")\n",
    "            top_noun_docs = re_rank_doc(question, retrieved_noun_docs_str, CROSS_ENCODER_MODEL, top_entity_count)\n",
    "            logging.info(f\"Noun documents after Re-ranking is {top_noun_docs}\")\n",
    "\n",
    "            logging.info(\"Filtering noun documents.\")\n",
    "            filtered_noun_docs_str = filter_context(question, top_noun_docs, CODE_BISON, True)\n",
    "            logging.info(f\"Length of noun documents after filtering is {len(filtered_noun_docs_str)}\")\n",
    "            logging.info(f\"Noun documents after filtering is {filtered_noun_docs_str}\")\n",
    "\n",
    "            if filtered_noun_docs_str:\n",
    "                noun_doc_str = '\\n\\n'.join([str(doc) for doc in filtered_noun_docs_str])\n",
    "\n",
    "        if noun_doc_str:\n",
    "            noun_doc_str_prompt = \"\\n\\n\" + \"Below is the possible relevant SQL query and query result for the user's question: \" + \"\\n\\n\" + noun_doc_str\n",
    "        else:\n",
    "            noun_doc_str_prompt = \"\"\n",
    "\n",
    "        output = top_tabel_schema_str + noun_doc_str_prompt\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error occurred while getting documents: {e}\")\n",
    "        output = \"An error occurred while processing your request.\"\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# Arabic NLP\n",
    "########################################\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the stopwords and punkt tokenizer models if not already downloaded\n",
    "try:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "    logging.info(\"NLTK stopwords and tokenizer models downloaded successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error downloading NLTK resources: {e}\")\n",
    "\n",
    "# Define the list of Arabic stop words\n",
    "arabic_stopwords = set(stopwords.words('arabic'))\n",
    "\n",
    "# Function to remove stop words from a given text\n",
    "def remove_stopwords(text):\n",
    "    try:\n",
    "        # Modify the regex to include Arabic characters and numbers\n",
    "        text = re.sub(r'[^\\u0600-\\u06FFa-zA-Z0-9\\s]', '', text)\n",
    "        logging.info(\"Text cleaned of non-Arabic characters.\")\n",
    "\n",
    "        words = nltk.word_tokenize(text)\n",
    "        logging.info(f\"Tokenized text into {len(words)} words.\")\n",
    "\n",
    "        filtered_words = [word for word in words if word not in arabic_stopwords]\n",
    "        logging.info(f\"Removed stopwords, resulting in {len(filtered_words)} words.\")\n",
    "\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error occurred while removing stopwords: {e}\")\n",
    "        return text  # Return the original text if an error occurs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "#############################\n",
    "\"\"\"\n",
    "String parser for new translated query chain\n",
    "\"\"\"\n",
    "#############################\n",
    "\n",
    "class CustomOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        try:\n",
    "            # Split the text to extract translated text and entities\n",
    "            translated_text_part, entities_part = text.split(\"Entities used:\", 1)\n",
    "            translated_text = translated_text_part.replace(\"Translated Text:\", \"\").strip()\n",
    "            entities = [entity.strip() for entity in entities_part.split(\",\") if entity.strip()]\n",
    "            logging.info(\"Parsed translated text and entities successfully.\")\n",
    "            return {\"translated_text\": translated_text, \"entities\": entities}\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in CustomOutputParser: {e}\")\n",
    "            return {\"translated_text\": text, \"entities\": []}\n",
    "\n",
    "#############################\n",
    "\"\"\"\n",
    "Dict parser for NER chain \n",
    "\"\"\"\n",
    "#############################\n",
    "\n",
    "class DictOutputParser(BaseOutputParser):\n",
    "    def parse(self, output):\n",
    "        if isinstance(output, str):\n",
    "            match = re.search(r'{.*}', output, re.DOTALL)\n",
    "            if match:\n",
    "                dictionary_str = match.group(0)\n",
    "                try:\n",
    "                    dictionary = ast.literal_eval(dictionary_str)\n",
    "                    logging.info(\"Parsed dictionary successfully.\")\n",
    "                    return dictionary\n",
    "                except (SyntaxError, ValueError) as e:\n",
    "                    logging.error(f\"Error parsing dictionary: {e}\")\n",
    "                    return {}\n",
    "        return {}\n",
    "\n",
    "    def parse_result(self, result):\n",
    "        if 'text' in result:\n",
    "            return self.parse(result['text'])\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Helper Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## RAG Filter Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Filter RAG documents\n",
    "######################################\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.chains import LLMChain\n",
    "\n",
    "def filter_context(question, context, llm, entity_query=False, entity=False, fuzzy_entity=False):\n",
    "    \"\"\"\n",
    "    Filters relevant contexts based on the given question using a specified LLM.\n",
    "\n",
    "    Parameters:\n",
    "    - question: The question to evaluate relevance against.\n",
    "    - context: A list of context strings to filter.\n",
    "    - llm: The language model to use for processing.\n",
    "    - entity_query: Flag to indicate if relevance is for SQL queries.\n",
    "    - entity: Flag to indicate if relevance is for entity presence in context.\n",
    "    - fuzzy_entity: Flag to indicate if relevance is for entity presence in a question.\n",
    "\n",
    "    Returns:\n",
    "    - List of relevant contexts that are deemed relevant to the question.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define prompt templates for different scenarios\n",
    "    prompt_templates = {\n",
    "        'entity_query': \"You are an assistant that determines the relevance of the SQL query and its result to a given question.\\n\\nQuestion: {question}\\n\\n{context}\\n\\nIs this query relevant to the question above? Answer 'yes' or 'no'.\",\n",
    "        'entity': \"You are an assistant that determines the presence of the Entity in a given Context.\\n\\nEntity: {question}\\n\\nContext: {context}\\n\\nIs this Entity present in the context above? Answer 'yes' or 'no'.\",\n",
    "        'fuzzy_entity': \"You are an assistant that determines the presence of the Entity in a given question.\\n\\nQuestion: {question}\\n\\nEntity: {context}\\n\\nIs this Entity present in the question above? Answer 'yes' or 'no'.\",\n",
    "        'default': \"You are an assistant that determines the relevance of table schema to a given question.\\n\\nQuestion: {question}\\n\\nTable: {context}\\n\\nIs this table relevant to the question above? Answer 'yes' or 'no'.\"\n",
    "    }\n",
    "    \n",
    "    prompt_key = 'entity_query' if entity_query else 'entity' if entity else 'fuzzy_entity' if fuzzy_entity else 'default'\n",
    "    \n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"question\", \"context\"],\n",
    "        template=prompt_templates[prompt_key]\n",
    "    )\n",
    "\n",
    "    filter_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    try:\n",
    "        inputs = [{\"question\": question, \"context\": ctx} for ctx in context]\n",
    "        results = filter_chain.apply(inputs)\n",
    "\n",
    "        # Extract relevant contexts\n",
    "        relevant_contexts = [ctx[\"context\"] for ctx, res in zip(inputs, results) if 'yes' in res[\"text\"].lower()]\n",
    "        logging.info(f\"Filtered relevant contexts: {len(relevant_contexts)} found.\")\n",
    "        return relevant_contexts\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in filtering context: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Adv Translator chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def translator_with_rag(llm, text, tgt, rag_result, translated_query):\n",
    "    \"\"\"\n",
    "    Translates the user query based on rough translation and possible entities using a Language Model (LLM).\n",
    "    \n",
    "    Parameters:\n",
    "    - llm: The language model used for translation.\n",
    "    - text: The original user query.\n",
    "    - tgt: The target language for translation.\n",
    "    - rag_result: Retrieved documents or entities used for translation.\n",
    "    - translated_query: The initial rough translation of the query.\n",
    "    \n",
    "    Returns:\n",
    "    - result: A more accurate translated query based on the given inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(\"Starting the translation process with RAG\")\n",
    "    logging.debug(f\"Input text: {text}\")\n",
    "    logging.debug(f\"Target language: {tgt}\")\n",
    "    logging.debug(f\"RAG result: {rag_result}\")\n",
    "    logging.debug(f\"Rough translation: {translated_query}\")\n",
    "\n",
    "    try:\n",
    "        # Define the prompt template for LLM\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            \"\"\"You are an expert Translator especially in English and Arabic.\n",
    "            The query can be related to Bills, Daily Sales, Dispenser Configurations, Fuel Requests, Fuel Types, Locations, Nozzles, Ports, Regions, Shifts, Fuel Stations, Tenants, Transactions, Users.\n",
    "            \n",
    "            Below provided are the original user query, query translated roughly to {tgt}, possible entities in the query and its translations in {tgt}. \n",
    "            Return more accurate translated query.\n",
    "            \n",
    "            Instructions:\n",
    "            - Use the translation of an entity only if there is a good match. \n",
    "            - Do not alter or combine entities. Use them as they are.\n",
    "            - Do not use entities if there is no good match.\n",
    "\n",
    "            Original user query:\n",
    "            {text}\n",
    "            \n",
    "            Roughly translated query:\n",
    "            {translated_query}\n",
    "\n",
    "            Possible entities:\n",
    "            {rag_result}\n",
    "\n",
    "            Result:\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        # Initialize the output parser and LLM chain\n",
    "        output_parser = CustomOutputParser()\n",
    "        chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)\n",
    "\n",
    "        # Run the chain to get the translation result\n",
    "        result = chain.run({\n",
    "            \"tgt\": tgt,\n",
    "            \"text\": text,\n",
    "            \"rag_result\": rag_result,\n",
    "            \"translated_query\": translated_query\n",
    "        })\n",
    "\n",
    "        logging.info(\"Translation process completed successfully\")\n",
    "        logging.debug(f\"Translation result: {result}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during translation: {e}\")\n",
    "        return {\"error\": f\"Failed to translate query: {str(e)}\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## NER chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "# Identify entities in the user query \n",
    "##################################\n",
    "\n",
    "def ner_identifier(llm, query):\n",
    "    \"\"\"\n",
    "    Identifies named entities in the given user query using a Language Model (LLM).\n",
    "    \n",
    "    Parameters:\n",
    "    - llm: The language model used for named entity recognition (NER).\n",
    "    - query: The user query where named entities are to be identified.\n",
    "    \n",
    "    Returns:\n",
    "    - entity_list: A comma-separated string of identified entities.\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(\"Starting NER identification process\")\n",
    "    logging.debug(f\"User query: {query}\")\n",
    "\n",
    "    try:\n",
    "        # Define the NER prompt template for LLM\n",
    "        ner_prompt = PromptTemplate.from_template(\n",
    "            \"\"\"You are an expert at identifying Entities.\n",
    "            Given the original user query, identify the possible named entities in the query and return all entities (names, locations, addresses, etc.).\n",
    "\n",
    "            Original user query:\n",
    "            {text}\n",
    "\n",
    "            Result (just the entities separated by commas):\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        # Initialize the output parser and LLM chain\n",
    "        output_parser = CommaSeparatedListOutputParser()\n",
    "        chain = LLMChain(prompt=ner_prompt, llm=llm, output_parser=output_parser)\n",
    "\n",
    "        # Run the chain to get the identified entities\n",
    "        result = chain.invoke({\"text\": query})\n",
    "\n",
    "        # Extract and return the list of entities\n",
    "        entity_list = result['text']\n",
    "        logging.info(\"NER identification completed successfully\")\n",
    "        logging.debug(f\"Identified entities: {entity_list}\")\n",
    "\n",
    "        return entity_list\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during NER identification: {e}\")\n",
    "        return {\"error\": f\"Failed to identify entities: {str(e)}\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "# Identify entities in the user query \n",
    "##################################\n",
    "\n",
    "def ner_translator(llm, query):\n",
    "    \"\"\"\n",
    "    Translates an Arabic entity name into English, focusing on pronunciation rather than meaning.\n",
    "    \n",
    "    Parameters:\n",
    "    - llm: The language model used for translation.\n",
    "    - query: The Arabic entity name to be translated.\n",
    "    \n",
    "    Returns:\n",
    "    - entity_list: A string representing the translated entity name, or 'None' if it cannot be translated.\n",
    "    \"\"\"\n",
    "    \n",
    "    logging.info(\"Starting Arabic-to-English entity translation\")\n",
    "    logging.debug(f\"Arabic entity provided for translation: {query}\")\n",
    "\n",
    "    try:\n",
    "        # Define the translation prompt template for LLM\n",
    "        ner_tran_prompt = PromptTemplate.from_template(\n",
    "            \"\"\"Given an entity name in Arabic, translate it to English. Do not translate its meaning; translate its pronunciation.\n",
    "            If you can't, just return 'None'.\n",
    "\n",
    "            Arabic entity:\n",
    "            {text}\n",
    "\n",
    "            Result (just the translated Arabic entity):\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        # Initialize the output parser and LLM chain\n",
    "        output_parser = CommaSeparatedListOutputParser()\n",
    "        chain = LLMChain(prompt=ner_tran_prompt, llm=llm, output_parser=output_parser)\n",
    "\n",
    "        # Run the chain to get the translated entity\n",
    "        result = chain.invoke({\"text\": query})\n",
    "\n",
    "        # Extract and return the translation result\n",
    "        entity_list = result['text']\n",
    "        logging.info(\"Translation process completed successfully\")\n",
    "        logging.debug(f\"Translated entity: {entity_list}\")\n",
    "\n",
    "        return entity_list\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during entity translation: {e}\")\n",
    "        return \"None\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85Zha4ZCQFSU",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Get SQL query chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert dictionary to JSON formatted string\n",
    "def _dict_to_json(x: dict) -> str:\n",
    "    return \"```\\n\" + json.dumps(x, indent=4) + \"\\n```\"\n",
    "\n",
    "# Clean SQL query by removing unwanted formatting\n",
    "def clean_query(query: str) -> str:\n",
    "    return query.replace(\"`sql\\n\", \"\").replace(\"```sql\", \"\").replace(\"```\", \"\").replace(\"``\", \"\")\n",
    "\n",
    "# Format SQL or Error prompt\n",
    "def format_prompt(template, context=None, chat_history=None, question=None, error=None, result=None, query=None, entities=None, answer=None):\n",
    "    \"\"\"Helper function to format the SQL or Error prompt.\"\"\"\n",
    "    return template.format(\n",
    "        context=context or '',\n",
    "        chat_history=chat_history or '',\n",
    "        question=question or '',\n",
    "        error=error or '',\n",
    "        query=query or '',\n",
    "        result=result or '',\n",
    "        entities=entities or '',\n",
    "        answer=answer or ''\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute SQL query\n",
    "def execute_query(query, fetch_all=True):\n",
    "    connection = None\n",
    "    result = None\n",
    "    try:\n",
    "        logging.info(f\"Executing query: {query}\")\n",
    "        connection = connect_db() \n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(query)\n",
    "\n",
    "        if fetch_all:\n",
    "            result = cursor.fetchall()\n",
    "        else:\n",
    "            result = cursor.fetchone()\n",
    "\n",
    "        logging.info(\"Query executed successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error executing query: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if connection:\n",
    "            connection.close()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WEdgWhHnQJBe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from operator import itemgetter\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "SQL_PROMPT = \"\"\"You are a PostgreSQL expert.\n",
    "\n",
    "Create a PostgreSQL query based on the following schema and user input.\n",
    "\n",
    "Schema (JSON format):\n",
    "{context}\n",
    "\n",
    "Conversation so far:\n",
    "{chat_history}\n",
    "\n",
    "User query:\n",
    "{question}\n",
    "\n",
    "Instructions:\n",
    "- Return only the PostgreSQL code.\n",
    "- Use standard PostgreSQL syntax.\n",
    "- Do not use backticks or any markup.\n",
    "- Use double quotes for schema names and table names.\n",
    "- Use double quotes for column names.\n",
    "- Use single quotes for values.\n",
    "\n",
    "SQL query:\n",
    "\"\"\"\n",
    "\n",
    "#######################################################################\n",
    "ERROR_PROMPT = \"\"\"Here are the PostgreSQL queries and their errors:\n",
    "\n",
    "{error}\n",
    "\n",
    "Modify the query to fix the errors using the schema and context below.\n",
    "\n",
    "Schema (JSON format):\n",
    "{context}\n",
    "\n",
    "Conversation so far:\n",
    "{chat_history}\n",
    "\n",
    "User query:\n",
    "{question}\n",
    "\n",
    "Instructions:\n",
    "- Return only the PostgreSQL code.\n",
    "- Use standard PostgreSQL syntax.\n",
    "- Do not use backticks or any markup.\n",
    "- Use double quotes for schema names and table names.\n",
    "- Use double quotes for column names.\n",
    "- Use single quotes for values.\n",
    "\n",
    "Corrected SQL query:\n",
    "\"\"\"\n",
    "\n",
    "# Generate SQL query using LLM and retry if errors occur\n",
    "def sql_query_generator(input, config, palm_max_attempts=3, gemini_max_attempts=2):\n",
    "    \"\"\"\n",
    "    Generates a PostgreSQL query using an LLM model and retries corrections based on errors.\n",
    "    \n",
    "    Parameters:\n",
    "    - input: Input dictionary containing the user's query, entities, and chat history.\n",
    "    - config: Configuration parameters.\n",
    "    - palm_max_attempts: Max attempts using the first model (LLM).\n",
    "    - gemini_max_attempts: Max attempts using the second model (fallback model).\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with the final query and results, or error information if failed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        docs = get_documents(input['input'], input['entities'])\n",
    "        question = itemgetter(\"input\")(input)\n",
    "        chat_history = itemgetter(\"chat_history\")(input)\n",
    "        formatted_prompt = format_prompt(template=SQL_PROMPT, context=docs, chat_history=chat_history, question=question)\n",
    "\n",
    "        max_attempts = palm_max_attempts + gemini_max_attempts\n",
    "        attempts = 0\n",
    "        error_list = []\n",
    "\n",
    "        while attempts <= max_attempts:\n",
    "            logging.info(f\"Attempt {attempts+1} of {max_attempts}\")\n",
    "            \n",
    "            if attempts < palm_max_attempts:\n",
    "                logging.debug(f\"Using Chat Code Bison for sql query generation\")\n",
    "                logging.debug(f\"Prompt: {formatted_prompt}\")\n",
    "                query = CODE_BISON.invoke(formatted_prompt).content\n",
    "            else:\n",
    "                logging.debug(f\"Using Gemini Pro for sql query generation\")\n",
    "                logging.debug(f\"Prompt: {formatted_prompt}\")\n",
    "                query = GEMINI_PRO(formatted_prompt)\n",
    "            \n",
    "            cleaned_query = clean_query(query)\n",
    "            logging.debug(f\"Cleaned query: {cleaned_query}\")\n",
    "            \n",
    "            try:\n",
    "                # Attempt to execute the query\n",
    "                query_result = execute_query(cleaned_query)\n",
    "                logging.info(f\"Query executed successfully: {cleaned_query}\")\n",
    "                return {\"query\": cleaned_query, \"result\": query_result, \"context\": docs, \"question\": question}\n",
    "            \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error executing query: {e}\")\n",
    "                error_message = f\"Query: {cleaned_query}\\nError: {str(e)}\"\n",
    "                error_list.append(error_message)\n",
    "                errors_so_far = \"\\n\".join(error_list)\n",
    "                \n",
    "                attempts += 1\n",
    "                if attempts < palm_max_attempts:\n",
    "                    # Generate a corrected query using the error message\n",
    "                    formatted_error_prompt = format_prompt(ERROR_PROMPT, docs, chat_history, question, error=errors_so_far)\n",
    "                    formatted_prompt = formatted_error_prompt\n",
    "                elif attempts == palm_max_attempts:\n",
    "                    # Retry with initial prompt\n",
    "                    formatted_prompt = format_prompt(template=SQL_PROMPT, context=docs, chat_history=chat_history, question=question)\n",
    "                elif attempts <= max_attempts:\n",
    "                    # Use the error prompt for the fallback model\n",
    "                    formatted_error_prompt = format_prompt(ERROR_PROMPT, docs, chat_history, question, error=errors_so_far)\n",
    "                    formatted_prompt = formatted_error_prompt\n",
    "                else:\n",
    "                    logging.error(\"Failed to generate a correct query after multiple attempts.\")\n",
    "                    return {\n",
    "                        \"error\": \"Failed to generate a correct query after multiple attempts.\",\n",
    "                        \"query\": cleaned_query,\n",
    "                        \"error_messages\": error_list,\n",
    "                        \"query_result\": None\n",
    "                    }\n",
    "        return {\"error\": \"Max attempts reached, no valid query.\", \"query_result\": None}\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\")\n",
    "        return {\"error\": \"An unexpected error occurred.\", \"query_result\": None}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7mvJMXqOML9",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Interpret the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtbGdTCDUlLz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "INTERPRET_PROMPT = \"\"\"You are a PostgreSQL expert and skilled in extracting data from CSV.\n",
    "\n",
    "A user asked:\n",
    "{question}\n",
    "\n",
    "The SQL query run was:\n",
    "{query}\n",
    "\n",
    "The query result in CSV format:\n",
    "{result}\n",
    "\n",
    "Based on this result, provide a brief answer to the user's question.\n",
    "\n",
    "Instructions:\n",
    "- Provide only the answer, without explaining how it was obtained.\n",
    "- Extract the answer solely from the query result.\n",
    "- Do not include the user question in your response.\n",
    "- If the answer cannot be found, respond with \"I cannot find the answer\" and include a brief explanation with relevant details.\n",
    "\"\"\"\n",
    "\n",
    "def interpret_data(input, config):\n",
    "    \"\"\"\n",
    "    Interprets data based on the input query and context.\n",
    "\n",
    "    Parameters:\n",
    "    - input: Dictionary containing the query, context, question, and result.\n",
    "    - config: Configuration parameters (if any).\n",
    "\n",
    "    Returns:\n",
    "    - interpreted_result: The interpreted result of the BigQuery data.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        logging.info(\"Interpreting data...\")\n",
    "\n",
    "        # Extract query and context from the input\n",
    "        query = itemgetter(\"query\")(input)\n",
    "        context = itemgetter(\"context\")(input)\n",
    "        question = itemgetter(\"question\")(input)\n",
    "        result = itemgetter(\"result\")(input)\n",
    "\n",
    "        logging.info(f\"Extracted input values: Query: {query}, Question: {question}\")\n",
    "        logging.debug(f\"Query: {query}\")\n",
    "        logging.debug(f\"Question: {question}\")\n",
    "        logging.debug(f\"Context : {context}\")\n",
    "\n",
    "\n",
    "        # Format the prompt for data interpretation\n",
    "        prompt_template = format_prompt(template=INTERPRET_PROMPT, question=question, result=result, query=query)\n",
    "        logging.debug(f\"Prompt: {prompt_template}\")\n",
    "\n",
    "        # Get the interpreted result\n",
    "        interpreted_result = INTERPRETOR_MODEL.invoke(prompt_template)\n",
    "        logging.info(\"Interpreted result retrieved successfully.\")\n",
    "\n",
    "        return interpreted_result.content\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error interpreting data: {e}\")\n",
    "        return {\"error\": \"Failed to interpret data\", \"details\": str(e)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Translator English to Arabic Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def translate_en_to_ar(query, query_response, entities_list, config):\n",
    "    \"\"\"\n",
    "    Translates an English query result and response into Arabic, considering entity mappings.\n",
    "\n",
    "    Parameters:\n",
    "    - query: A dictionary containing the question, query, and query result.\n",
    "    - query_response: The result to be translated.\n",
    "    - entities_list: A list of possible entities that need special handling for translation.\n",
    "    - config: Configuration object containing additional parameters (e.g., translation models).\n",
    "\n",
    "    Returns:\n",
    "    - translation_result.content: Translated response in Arabic.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define the translation prompt template\n",
    "        TRANSLATE_PROMPT = \"\"\"You are an expert English to Arabic Translator.\n",
    "\n",
    "        A user asked this question:\n",
    "        {question}\n",
    "\n",
    "        To find the answer, the following SQL query was run in BigQuery:\n",
    "        ```\n",
    "        {query}\n",
    "        ```\n",
    "\n",
    "        The result of that query was the following table in CSV format:\n",
    "        ```\n",
    "        {result}\n",
    "        ```\n",
    "\n",
    "        {entities}\n",
    "\n",
    "        This is the brief answer to the user question:\n",
    "        ```\n",
    "        {answer}\n",
    "        ```\n",
    "\n",
    "        Using all the above information, translate the answer from English to Arabic.\n",
    "\n",
    "        Follow these restrictions strictly:\n",
    "        - Use only the given information to translate.\n",
    "        - If the entities in the answer are present in the given possible English : Arabic entity translation, use the Arabic entity as is.\n",
    "        - Just write the answer, omit the question from your answer, this is a chat, just provide the answer.\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info(\"Starting translate_en_to_ar function.\")\n",
    "\n",
    "        # Extract relevant fields from the query\n",
    "        question = query.get(\"question\", \"\")\n",
    "        sql_query = query.get(\"query\", \"\")\n",
    "        query_result = query.get(\"result\", \"\")\n",
    "        answer = query_response\n",
    "\n",
    "        # Handle entity mappings if available\n",
    "        entities = None\n",
    "        if entities_list:\n",
    "            entity_maps = []\n",
    "            for entity in entities_list:\n",
    "                for entity_map in entity_translate_map:\n",
    "                    if entity in entity_map:\n",
    "                        entity_maps.append({entity: entity_map[entity]})\n",
    "\n",
    "            # Prepare the entity translation mapping\n",
    "            if entity_maps:\n",
    "                entity_map_str = '\\n'.join([f\"{en}: {ar}\" for mapping in entity_maps for en, ar in mapping.items()])\n",
    "                entities = f\"\\n\\nHere are some entities in English and their Arabic translations that might exist in the answer:\\n```\\n{entity_map_str}\\n```\"\n",
    "        \n",
    "        logging.debug(f\"Entity mappings: {entities}\")\n",
    "\n",
    "        # Format the prompt for translation\n",
    "        prompt_template = format_prompt(\n",
    "            template=TRANSLATE_PROMPT,\n",
    "            question=question,\n",
    "            result=query_result,\n",
    "            query=sql_query,\n",
    "            entities=entities,\n",
    "            answer=answer\n",
    "        )\n",
    "        logging.debug(f\"Formatted prompt: {prompt_template}\")\n",
    "\n",
    "        # Invoke the translation model\n",
    "        translation_result = CHAT_BISON.invoke(prompt_template)\n",
    "        logging.info(\"Translation result successfully retrieved.\")\n",
    "        \n",
    "        return translation_result.content\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during translation: {e}\", exc_info=True)\n",
    "        return {\"error\": \"Failed to translate the query result.\", \"details\": str(e)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pj3XlntDVzV2",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Math tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R8X1LGzzXUcw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import LLMMathChain\n",
    "from langchain.tools import Tool\n",
    "\n",
    "math_chain = LLMMathChain.from_llm(llm=AGENT_MODEL)\n",
    "math_tool = Tool(\n",
    "  name=\"Calculator\",\n",
    "  description=\"Useful for when you need to answer questions about math.\",\n",
    "  func=math_chain.run,\n",
    "  coroutine=math_chain.arun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Advanced translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def replace_words(text, replacements):\n",
    "    words = text.split()\n",
    "    new_words = [replacements.get(word, word) for word in words]\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "# Define the dictionary for replacements\n",
    "replacements = {\n",
    "    \"operations\": \"transactions\",\n",
    "    \"sessions\": \"transactions\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "#############################\n",
    "# Translator with RAG\n",
    "#############################\n",
    "\n",
    "do_print = False\n",
    "def translate_text_v2(llm, text: str, tgt: str):\n",
    "    \"\"\"\n",
    "    Translates a given text into the target language, with entity recognition and RAG (Retrieve and Generate) support.\n",
    "    \n",
    "    Parameters:\n",
    "    - llm: Large language model used for translation tasks.\n",
    "    - text: The input text to be translated.\n",
    "    - tgt: The target language code.\n",
    "    \n",
    "    Returns:\n",
    "    - new_query: The translated text or the original text if no translation was needed.\n",
    "    - translated_entities_used: List of entities translated during the process.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        translate_client = translate.Client()\n",
    "\n",
    "        if isinstance(text, bytes):\n",
    "            text = text.decode(\"utf-8\")\n",
    "        \n",
    "        detected_source_language = translate_client.detect_language(text)[\"language\"]\n",
    "        logging.info(f\"Detected source language: {detected_source_language}\")\n",
    "\n",
    "        logging.debug(f\"Source languages: {detected_source_language}.\")\n",
    "        logging.debug(\"Target languages: {tgt}.\")\n",
    "        logging.debug(\"Text: {text}.\")\n",
    "\n",
    "        translated_entities_used = []\n",
    "        if detected_source_language == tgt:\n",
    "            logging.info(\"Source and target languages are the same. No translation needed.\")\n",
    "\n",
    "\n",
    "\n",
    "            return text, translated_entities_used\n",
    "\n",
    "        # Translate the text if necessary\n",
    "        translated_query = translate_text(text, tgt)\n",
    "        logging.info(\"Basic translation sucessful.\")\n",
    "        logging.debug(\"Basic translated text: {translated_query}.\")\n",
    "\n",
    "\n",
    "        # Initialize variables for storing NER results and entity matching\n",
    "        ar_ner_res, en_ner_res, query_match_entities_list = [], [], []\n",
    "        ar_ner_res_list, en_ner_res_list, ner_match_entities_list = [], [], []\n",
    "\n",
    "        ############################# Query RAG and Filtering ##############################\n",
    "\n",
    "        query_rag = entity_translate_retriever.get_relevant_documents(text)\n",
    "        query_rag_list = list({doc.page_content for doc in query_rag})\n",
    "\n",
    "        if query_rag_list:\n",
    "            query_re_rank = re_rank_doc(text, query_rag_list, CROSS_ENCODER_MODEL, 2)\n",
    "            query_filter = filter_context(text, query_rag_list, INTERPRETOR_MODEL)\n",
    "\n",
    "            logging.debug(f\"Re-ranked query: {query_re_rank}\")\n",
    "            logging.debug(f\"Filtered query: {query_filter}\")\n",
    "\n",
    "        ############################# Fuzzy Search for Entities ############################\n",
    "\n",
    "        query_cleaned = remove_stopwords(text)\n",
    "        query_top_match = get_matches(query_cleaned.split(), all_translated_words)\n",
    "\n",
    "        if query_top_match:\n",
    "            query_re_ranked_match = re_rank_doc(text, query_top_match, CROSS_ENCODER_MODEL, 3)\n",
    "            query_re_ranked_match_entities = map_fuzzy_to_entity_translator_context(query_re_ranked_match, translated_texts_dict, entity_translate_map)\n",
    "            query_filtered_match = filter_context(text, query_top_match, INTERPRETOR_MODEL, fuzzy_entity=True)\n",
    "\n",
    "            query_match_entities = map_fuzzy_to_entity_translator_context(query_filtered_match, translated_texts_dict, entity_translate_map)\n",
    "            query_match_entities_list = [str(entity) for entity in query_match_entities]\n",
    "            logging.debug(f\"Fuzzy matched entities: {query_match_entities_list}\")\n",
    "\n",
    "        ############################# Named Entity Recognition (NER) ########################\n",
    "\n",
    "        ar_ner = ner_identifier(CHAT_BISON, text)\n",
    "        if ar_ner:\n",
    "            en_ner = [translated for ner in ar_ner for translated in ner_translator(CHAT_BISON, ner)]\n",
    "            logging.debug(f\"Arabic NER: {ar_ner}\")\n",
    "            logging.debug(f\"English NER: {en_ner}\")\n",
    "\n",
    "            # # Handle Arabic NER\n",
    "            # for ner_item in ar_ner:\n",
    "            #     process_ner_entities(ner_item, ar_ner_res, ar_ner_res_list, \"Arabic\")\n",
    "\n",
    "            # # Handle English NER\n",
    "            # for ner_item in en_ner:\n",
    "            #     process_ner_entities(ner_item, en_ner_res, en_ner_res_list, \"English\")\n",
    "\n",
    "        entity_count=1\n",
    "        if ar_ner:\n",
    "            entity_count = len(ar_ner)\n",
    "        \n",
    "            ner_res = []\n",
    "            ner = []\n",
    "            ner = ar_ner + en_ner\n",
    "            # ner.append(en_ner)\n",
    "            start_time = time.time()\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                ar_future = executor.submit(process_ner, ar_ner, 'ar')\n",
    "                en_future = executor.submit(process_ner, en_ner, 'en')\n",
    "                \n",
    "                ar_ner_res_list = ar_future\n",
    "                en_ner_res_list = en_future\n",
    "                \n",
    "                logging.debug(f\"Filtered Arabic NER: {ar_ner_res_list}\")\n",
    "                logging.debug(f\"Filtered English NER: {en_ner_res_list}\")\n",
    "\n",
    "                \n",
    "                \n",
    "        ############################# Fuzzy Search for NER Entities #########################\n",
    "\n",
    "        ner_split_flattened = [item for sublist in [remove_stopwords(ele).split() for ele in ar_ner] for item in sublist]\n",
    "        if '' in ner_split_flattened:\n",
    "            ner_split_flattened.remove('')\n",
    "\n",
    "        ner_top_matches = get_matches(ner_split_flattened, all_translated_words, 5)\n",
    "        if ner_top_matches:\n",
    "            process_fuzzy_matches(ner_top_matches, ar_ner, en_ner_res_list, ar_ner_res_list)\n",
    "\n",
    "        ############################# Final RAG and Fuzzy Result ############################\n",
    "\n",
    "        entities_list = list(set(query_filter + query_match_entities_list + en_ner_res_list + ar_ner_res_list + ner_match_entities_list))\n",
    "        logging.info(f\"Final list of entities: {entities_list}\")\n",
    "\n",
    "        ############################# LLM Chain for Translation #############################\n",
    "        \n",
    "        result = translator_with_rag(CHAT_BISON, text, tgt, entities_list, translated_query)\n",
    "        new_query = replace_words(result['translated_text'], replacements)\n",
    "        logging.debug(f\"Translation with adv translator with out replacer: {result}\")\n",
    "        logging.debug(f\"Translation with replacer: {new_query}\")\n",
    "\n",
    "        translated_entities_used = result['entities']\n",
    "        \n",
    "        logging.info(\"Translation completed successfully.\")\n",
    "        return new_query, translated_entities_used\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during translation: {e}\", exc_info=True)\n",
    "        return {\"error\": \"Translation failed.\", \"details\": str(e)}\n",
    "\n",
    "\n",
    "def process_ner(ner, language):\n",
    "    ner_res = []\n",
    "\n",
    "    for ele in ner:\n",
    "        ############# RAG #############\n",
    "        ner_rag = entity_translate_retriever.get_relevant_documents(ele)\n",
    "        ner_rag_list = list({doc.page_content for doc in ner_rag})  # Unique results\n",
    "        \n",
    "        ############# Re-Rank and filter #############\n",
    "        if ner_rag_list:\n",
    "            # ner_re_rank = re_rank_doc(ele, ner_rag_list, CROSS_ENCODER_MODEL)\n",
    "            ner_filter = filter_context(ele, ner_rag_list, INTERPRETOR_MODEL, entity_query=False, entity=True)\n",
    "            ner_res.append(list(ner_filter))\n",
    "\n",
    "    ner_res_future = ner_res.result()\n",
    "    ner_res_list = sum(ner_res_future, [])\n",
    "\n",
    "    return ner_res_list\n",
    "\n",
    "\n",
    "def process_fuzzy_matches(ner_top_matches, ar_ner, en_ner_res_list, ar_ner_res_list):\n",
    "    \"\"\"\n",
    "    Helper function to process fuzzy matches for NER entities.\n",
    "    \n",
    "    Parameters:\n",
    "    - ner_top_matches: The top matches from the fuzzy search.\n",
    "    - ar_ner: Arabic NER results.\n",
    "    - en_ner_res_list: List to store English NER results.\n",
    "    - ar_ner_res_list: List to store Arabic NER results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ner_re_rank_match = re_rank_doc(ar_ner, ner_top_matches, CROSS_ENCODER_MODEL, len(ar_ner) * 2)\n",
    "        ner_rr_match_entities = map_fuzzy_to_entity_translator_context(ner_re_rank_match, translated_texts_dict, entity_translate_map)\n",
    "        ner_filtered_match = filter_context(ar_ner, ner_top_matches, INTERPRETOR_MODEL, fuzzy_entity=True)\n",
    "        ner_match_entities = map_fuzzy_to_entity_translator_context(ner_filtered_match, translated_texts_dict, entity_translate_map)\n",
    "\n",
    "        en_ner_res_list.extend(ner_rr_match_entities)\n",
    "        ar_ner_res_list.extend(ner_match_entities)\n",
    "\n",
    "        logging.debug(f\"Fuzzy NER matches processed successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing fuzzy NER matches: {e}\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AGENT_PROMPT = \"\"\"You are a very powerful assistant that can answer questions. \n",
    "\n",
    "ALWAYS USE user_question_tool to answer fuel, fuel stations, station locations, transcations and bills related questions.\n",
    "\n",
    "You can invoke the tool Calculator if you need to do mathematical operations.\n",
    "\n",
    "Always use the tools to try to answer the questions. Use the chat history for context. Never try to use any other external information.\n",
    "\n",
    "Assume that the user may write with misspellings, fix the spelling of the user before passing the question to any tool.\n",
    "\n",
    "Don't mention what tool you have used in your answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class UserQuestionToolInput(BaseModel):\n",
    "    input: int = Field(description=\"User natural language questions to be answered.\")\n",
    "    user_id: int = Field(description=\"User Id\")\n",
    "\n",
    "\n",
    "@tool(\"user-question-tool\", args_schema= UserQuestionToolInput)\n",
    "def user_question_tool(input: str, user_id: str ) -> str:\n",
    "    \"\"\"Answers natural language questions related to fuel, fuel stations, station locations, transcations and bills using data from database\"\"\"\n",
    "    \n",
    "    config = {'callbacks': [ConsoleCallbackHandler()]}\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Using user_question_tool for user: {user_id}\")\n",
    "        logger.info(f\"Using user_question_tool for user question: {input}\")\n",
    "\n",
    "\n",
    "        # Fetch user memory\n",
    "        user_memory = user_memories.get(user_id)\n",
    "        if not user_memory:\n",
    "            raise ValueError(\"User memory not found.\")\n",
    "        memory = user_memory.buffer_as_str.strip()\n",
    "\n",
    "        # Translate input to English\n",
    "        try:\n",
    "            translated_text, entities_list = translate_text_v2(CHAT_BISON, input, \"en\")\n",
    "            logger.info(f\"Translated text: {translated_text}, Entities: {entities_list}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during translation: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # Prepare query\n",
    "        question = {\"input\": translated_text, \"chat_history\": memory, \"entities\": entities_list}\n",
    "        \n",
    "        # Generate SQL query\n",
    "        try:\n",
    "            query = sql_query_generator(question, config)\n",
    "            logger.info(f\"Generated SQL query: {query}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in SQL query generation: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # Interpret query response\n",
    "        try:\n",
    "            query_response = interpret_data(query, config)\n",
    "            logger.info(f\"Query response: {query_response}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error interpreting query response: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # Translate query response back to Arabic\n",
    "        try:\n",
    "            translated_result = translate_en_to_ar(query, query_response, entities_list, config)\n",
    "            logger.info(f\"Translated result: {translated_result}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error translating result to Arabic: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # Save conversation context\n",
    "        try:\n",
    "            user_memory.save_context({\"input\": translated_text}, {\"output\": translated_result.strip()})\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving user memory: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        return translated_result.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred in user_question_tool: {str(e)}\")\n",
    "        return \"An error occurred while processing your request.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "user_memories = defaultdict(lambda: ConversationBufferWindowMemory(memory_key=\"chat_history\", k=50, return_messages=True))\n",
    "user_agents = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent, tool\n",
    "\n",
    "\n",
    "# Define the agent's system prompt\n",
    "AGENT_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a very powerful assistant that can answer questions using postgres.\"),\n",
    "    (\"placeholder\", \"{chat_history}\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\")\n",
    "])\n",
    "\n",
    "# List of tools available for the agent\n",
    "agent_tools = [math_tool, user_question_tool]\n",
    "\n",
    "def get_or_create_agent(user_id):\n",
    "    try:\n",
    "        # Check if the user agent already exists\n",
    "        if user_id in user_agents:\n",
    "            logging.info(f\"Reusing existing agent for user_id: {user_id}\")\n",
    "            return user_agents[user_id]\n",
    "\n",
    "        logging.info(f\"Creating new agent for user_id: {user_id}\")\n",
    "        # Get the memory associated with the user\n",
    "        agent_memory = user_memories.get(user_id)\n",
    "\n",
    "        if not agent_memory:\n",
    "            logging.error(f\"No memory found for user_id: {user_id}\")\n",
    "            raise ValueError(f\"Memory for user_id {user_id} not initialized.\")\n",
    "\n",
    "        # Create a new agent using create_tool_calling_agent\n",
    "        agent = create_tool_calling_agent(\n",
    "            llm=AGENT_MODEL,\n",
    "            tools=agent_tools,\n",
    "            prompt=AGENT_PROMPT\n",
    "        )\n",
    "\n",
    "        # Wrap the agent in an executor with settings like max iterations and verbose logging\n",
    "        agent_executor = AgentExecutor(\n",
    "            agent=agent,\n",
    "            tools=agent_tools,\n",
    "            memory=agent_memory,\n",
    "            max_iterations=5,\n",
    "            early_stopping_method='generate',\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        # Store the agent executor for future use\n",
    "        user_agents[user_id] = agent_executor\n",
    "        logging.info(f\"Agent successfully created for user_id: {user_id}\")\n",
    "\n",
    "        return agent_executor\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to create or retrieve agent for user_id: {user_id}. Error: {e}\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def process_user_query(user_id: str, query: str) -> str:\n",
    "    \"\"\"Process the user query and return the response.\"\"\"\n",
    "\n",
    "    # Validate inputs\n",
    "    if not user_id or not query:\n",
    "        logging.error(\"Invalid input: user_id and query cannot be empty.\")\n",
    "        raise ValueError(\"Both user_id and query must be provided.\")\n",
    "\n",
    "    try:\n",
    "        # Get or create an agent for the user\n",
    "        agent = get_or_create_agent(user_id)\n",
    "        logging.info(f\"Agent created for user_id: {user_id}\")\n",
    "\n",
    "        # Process the user query\n",
    "        response = agent({\"input\": query})\n",
    "        logging.info(f\"Query processed for user_id: {user_id}, query: {query}\")\n",
    "\n",
    "        # Extract the output from the response\n",
    "        output = response.get('output', \"No output returned.\")\n",
    "        \n",
    "        if output is None:\n",
    "            logging.warning(f\"No output found in response for user_id: {user_id}.\")\n",
    "            return \"No output found in response.\"\n",
    "\n",
    "        return output\n",
    "\n",
    "    except KeyError as e:\n",
    "        logging.error(f\"Key error while processing query: {str(e)}\")\n",
    "        return \"Error processing your query. Please try again.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred for user_id: {user_id}, query: {query}: {str(e)}\")\n",
    "        return \"An unexpected error occurred while processing your query.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_memories.clear()\n",
    "user_agents.clear()\n",
    "user_agents\n",
    "user_memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_id = \"user12\"  # Example user ID\n",
    "query_list = [\"     18   2024\", \"     18  2024 \", \"      18/05/2024\", \"           \", \"         2024 \", \"         \", \"          2024\", \"          2024 \", \"          \", \"         2024\", \"        2024\", \"       \", \"   \", \"   \", \"    \", \"     \", \"         \", \"      \", \"            \", \"            \", \"           \", \"       19  5  2024\", \"      19  2024 \", \"     19  2024 \", \"      5  2024    \", \"       2024    .\", \"        2024      \", \"      25  5  2024\", \"        25  2024 \", \"      25   2024\", \"     \", \"        \", \"       \", \"         5  2024      .\", \"          2024    \", \"         2024     \", \"              2024      .\", \"             2024      \", \"          5  2024       \", \"        2024     .\", \"       2024       .\", \"      2024      \", \"          2024           .\", \"      5  2024       \", \"       2024        \"]\n",
    "\n",
    "# query = \"Which stations have charging?\"\n",
    "# query = \"     \"\n",
    "# query = \"Which stations have restrooms?\"\n",
    "# query = \"Can you give me name of 2 stations?\"\n",
    "# query = \"what were the employee sales for the date 18th of May 2024?\"\n",
    "# query = \"     18  2024\"\n",
    "# query = \"what is the employee sales of the 1st 2 week of may 2024\"\n",
    "# query = \"How much fuel was requested on 1st week of may 2024\"\n",
    "# query = \"give me a brief summary on the transactions done in may 2024\"\n",
    "# query = \"Who approved the fuel requested on 19th may 2024\"\n",
    "# query = \"Tell me about the bill processed by Amal\"\n",
    "# query = \"     \"\n",
    "# query = \"What were the employee sales for the date 18th of May 2024?\"\n",
    "# query = \"         2024\"\n",
    "# query = \"     18   2024\"\n",
    "# query = \"        2024     .\" # What is the average transaction amount for May 2024? Include the average value of all transactions for the month.\n",
    "# query = \"         \" # Give me information about samara station\n",
    "# query =  \"        23  2024 \" # How many transactions were completed on May 25, 2024? \n",
    "# query = \"2024          \" # How much fuel was requested on the 1st week of May 2024?\n",
    "# query = \"no, give me average value of all transactions for the whole month, not only 18th\"\n",
    "query = \"     18  2024\"  # \"what were the employee sales for the date 18th of May 2024?\"\n",
    "response = process_user_query(user_id, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lanagchain set_llm_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_BISON = ChatVertexAI(model_name=\"codechat-bison\", max_output_tokens=1024)\n",
    "INTERPRETOR_MODEL = ChatVertexAI(max_output_tokens=1024)\n",
    "AGENT_MODEL = ChatVertexAI(max_output_tokens=1024)\n",
    "CHAT_BISON = ChatVertexAI(model_name=\"chat-bison@002\", max_output_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.caches import InMemoryCache\n",
    "from langchain_core.globals import set_llm_cache\n",
    "\n",
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 727 s, sys: 115 s, total: 842 s\n",
      "Wall time: 729 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' Machine learning (ML) is a subfield of artificial intelligence (AI) that gives computers the ability to learn without being explicitly programmed. ML algorithms are able to learn from and make decisions or predictions based on data. These algorithms are typically trained on large datasets and can be used for a variety of tasks, such as image recognition, natural language processing, and predictive analytics.')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "res = INTERPRETOR_MODEL.invoke(\"what is ML\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.71 ms, sys: 856 s, total: 10.6 ms\n",
      "Wall time: 2.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' Machine learning (ML) is a subfield of artificial intelligence (AI) that gives computers the ability to learn without being explicitly programmed. ML algorithms are able to learn from and make decisions or predictions based on data.\\n\\nThere are many different types of ML algorithms, each with its own strengths and weaknesses. Some of the most common types of ML algorithms include:\\n\\n* **Supervised learning:** In supervised learning, the ML algorithm is trained on a dataset of labeled data. This means that each data point in the dataset is associated with a known output. The ML algorithm learns to map input data to output data by finding patterns in the labeled data.\\n* **Unsupervised learning:** In unsupervised learning, the ML algorithm is trained on a dataset of unlabeled data. This means that each data point in the dataset does not have a known output. The ML algorithm learns to find patterns in the unlabeled data without being explicitly told what to look for.\\n* **Reinforcement learning:** In reinforcement learning, the ML algorithm learns by interacting with its environment. The ML algorithm receives rewards for good actions and punishments for bad actions, and it learns to take actions that maximize its rewards.\\n\\nML is used in a wide variety of applications, including:\\n\\n* **Image recognition:** ML algorithms can be used to identify objects in images. This technology is used in a variety of applications, such as facial recognition, object detection, and medical imaging.\\n* **Natural language processing:** ML algorithms can be used to understand and generate human language. This technology is used in a variety of applications, such as machine translation, spam filtering, and sentiment analysis.\\n* **Speech recognition:** ML algorithms can be used to convert spoken words into text. This technology is used in a variety of applications, such as voice control, dictation, and customer service.\\n* **Predictive analytics:** ML algorithms can be used to predict future events. This technology is used in a variety of applications, such as weather forecasting, stock market analysis, and fraud detection.\\n\\nML is a powerful tool that has the potential to revolutionize many industries. As ML algorithms become more sophisticated, we can expect to see even more innovative and groundbreaking applications of this technology in the years to come.')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "res = INTERPRETOR_MODEL.invoke(\"Tell me about ML\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 847 s, sys: 135 s, total: 982 s\n",
      "Wall time: 869 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' Machine learning (ML) is a subfield of artificial intelligence (AI) that gives computers the ability to learn without being explicitly programmed. ML algorithms are able to learn from and make decisions or predictions based on data.\\n\\nThere are many different types of ML algorithms, each with its own strengths and weaknesses. Some of the most common types of ML algorithms include:\\n\\n* **Supervised learning:** In supervised learning, the ML algorithm is trained on a dataset of labeled data. This means that each data point in the dataset is associated with a known output. The ML algorithm learns to map input data to output data by finding patterns in the labeled data.\\n* **Unsupervised learning:** In unsupervised learning, the ML algorithm is trained on a dataset of unlabeled data. This means that each data point in the dataset does not have a known output. The ML algorithm learns to find patterns in the unlabeled data without being explicitly told what to look for.\\n* **Reinforcement learning:** In reinforcement learning, the ML algorithm learns by interacting with its environment. The ML algorithm receives rewards for good actions and punishments for bad actions, and it learns to take actions that maximize its rewards.\\n\\nML is used in a wide variety of applications, including:\\n\\n* **Image recognition:** ML algorithms can be used to identify objects in images. This technology is used in a variety of applications, such as facial recognition, object detection, and medical imaging.\\n* **Natural language processing:** ML algorithms can be used to understand and generate human language. This technology is used in a variety of applications, such as machine translation, spam filtering, and sentiment analysis.\\n* **Speech recognition:** ML algorithms can be used to convert spoken words into text. This technology is used in a variety of applications, such as voice control, dictation, and customer service.\\n* **Predictive analytics:** ML algorithms can be used to predict future events. This technology is used in a variety of applications, such as weather forecasting, stock market analysis, and fraud detection.\\n\\nML is a powerful tool that has the potential to revolutionize many industries. As ML algorithms become more sophisticated, we can expect to see even more innovative and groundbreaking applications of this technology in the years to come.')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "res = INTERPRETOR_MODEL.invoke(\"Tell me about ML\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchainhub\n",
      "  Downloading langchainhub-0.1.21-py3-none-any.whl.metadata (659 bytes)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchainhub) (24.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchainhub) (2.32.3)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
      "  Downloading types_requests-2.32.0.20240914-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchainhub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchainhub) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchainhub) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchainhub) (2024.7.4)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchainhub)\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Downloading langchainhub-0.1.21-py3-none-any.whl (5.2 kB)\n",
      "Downloading types_requests-2.32.0.20240914-py3-none-any.whl (15 kB)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: urllib3, types-requests, langchainhub\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.19\n",
      "    Uninstalling urllib3-1.26.19:\n",
      "      Successfully uninstalled urllib3-1.26.19\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchainhub-0.1.21 types-requests-2.32.0.20240914 urllib3-2.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install langchainhub\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain/hub.py:80: DeprecationWarning: The `langchainhub sdk` is deprecated.\n",
      "Please use the `langsmith sdk` instead:\n",
      "  pip install langsmith\n",
      "Use the `pull_prompt` method.\n",
      "  resp: str = client.pull(owner_repo_commit)\n",
      "/opt/conda/lib/python3.10/site-packages/langchainhub/client.py:326: DeprecationWarning: The `langchainhub sdk` is deprecated.\n",
      "Please use the `langsmith sdk` instead:\n",
      "  pip install langsmith\n",
      "Use the `pull_prompt` method.\n",
      "  res_dict = self.pull_repo(owner_repo_commit)\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull(\"hwchase17/react\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['agent_scratchpad', 'input', 'tool_names', 'tools'], template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTEXT_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_PROMPT = \"\"\"You are a helpful assistant. Your task is to determine if the current user question is a follow-up to any previous questions.\n",
    "\n",
    "Current question:\n",
    "{question}\n",
    "\n",
    "Chat history:\n",
    "{chat_history}\n",
    "\n",
    "If the current question is indeed a follow-up, please formulate a new question that highlights the current question while incorporating relevant context from the related question and its answer.\n",
    "If the current question is not a  follow-up, return the current question as is.\n",
    "\n",
    "New question:\n",
    "\"\"\"\n",
    "\n",
    "# \" and its answer\" is optional\n",
    "\n",
    "# question adjusted for possible follow up \n",
    "adujusted_question = GEMINI_PRO(CONTEXT_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guard Rails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jwt\n",
    "import json\n",
    "from jwt import DecodeError, ExpiredSignatureError\n",
    "\n",
    "# Decoding JWT (without verification)\n",
    "def decode_jwt(token):\n",
    "    try:\n",
    "        # Decode the token to view its payload and header without verifying the signature\n",
    "        decoded = jwt.decode(token, options={\"verify_signature\": False})\n",
    "        \n",
    "        return decoded\n",
    "    except DecodeError as e:\n",
    "        return f\"Failed to decode token: {e}\"\n",
    "\n",
    "\n",
    "# Verifying JWT Signature\n",
    "def verify_jwt(token, pub_key):\n",
    "    try:\n",
    "        # Verifying the token with the public key (for RS256)\n",
    "        verified = jwt.decode(token, pub_key, algorithms=[\"RS256\"])\n",
    "        print(\"Verified JWT Payload: \", verified)\n",
    "        print(\"Verified JWT Header: \", verified.header)\n",
    "        return verified\n",
    "    except ExpiredSignatureError:\n",
    "        return \"Token has expired!\"\n",
    "    except DecodeError:\n",
    "        return \"Invalid token signature!\"\n",
    "\n",
    "def process_user_query(token: str, query: str) -> str: \n",
    "    \"\"\"Process the user query and return the response.\"\"\"\n",
    "    try:\n",
    "        if not token or not query:\n",
    "            logging.error(\"Token or query is missing.\")\n",
    "            raise ValueError(\"Token and query are required.\")\n",
    "        \n",
    "        # Decode JWT to get user data\n",
    "        user_data = decode_jwt(token)\n",
    "        \n",
    "        if 'TenantID' not in user_data or 'user_id' not in user_data:\n",
    "            logging.error(\"Invalid user data received from JWT.\")\n",
    "            raise ValueError(\"Invalid user data.\")\n",
    "        \n",
    "        tenant_id = user_data['TenantID']\n",
    "        user_id = user_data['user_id']\n",
    "        \n",
    "        logging.info(f\"Processing query for user_id: {user_id} with tenant_id: {tenant_id}\")\n",
    "\n",
    "        # Get or create the agent for the user\n",
    "        agent = get_or_create_agent(user_id)\n",
    "        \n",
    "        # Process the query using the agent\n",
    "        response = agent({\"input\": query})\n",
    "\n",
    "        if 'output' not in response:\n",
    "            logging.error(f\"No output found in response for user_id: {user_id}\")\n",
    "            raise ValueError(\"No output found in the response.\")\n",
    "        \n",
    "        output = response['output']\n",
    "        logging.info(f\"Query processed successfully for user_id: {user_id}\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing user query for token: {token}. Error: {e}\")\n",
    "        raise  # Re-raise the exception after logging\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calling_dummy_api(user_id):\n",
    "    return\n",
    "    \n",
    "def table_access_check():\n",
    "    # call API\n",
    "    permited_tables = calling_dummy_api(user_id)\n",
    "\n",
    "    # From RAG get table id, if restricted id are there remove that table\n",
    "    # from SQL query generated, check if table_id in query, if is there re generate query with restricted_prompt\n",
    "    # get the id of the user from tenant_id table, see how you can add filter to make sure only selected tenant and stations are seleted, \n",
    "        # Maybe use a function to that adds to WHERE clause if it exists if not, adds the WHERE clause   \n",
    "    # Get the sql result, make sure labels are there (wont work if aggregation are performed)\n",
    "        # and turn it into  DF,\n",
    "        # use it to filter out the ID  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Somefunc()\n",
    "end_time = time.time()\n",
    "print(f\"Time taken by Somefunc : {end_time - start_time} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
